<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>AsyncWay</title>
  <subtitle>Think &amp; Code</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://cyrusin.github.io/"/>
  <updated>2016-12-23T08:47:25.000Z</updated>
  <id>https://cyrusin.github.io/</id>
  
  <author>
    <name>Li Shuai</name>
    <email>lysh0526@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Redis相关的问题及应对策略</title>
    <link href="https://cyrusin.github.io/2016/12/16/redis-20161216/"/>
    <id>https://cyrusin.github.io/2016/12/16/redis-20161216/</id>
    <published>2016-12-16T09:46:50.000Z</published>
    <updated>2016-12-23T08:47:25.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-连接数过高"><a href="#1-连接数过高" class="headerlink" title="1. 连接数过高"></a>1. 连接数过高</h2><p>Redis连接数过高，且没有修改进程能打开的最大文件数，当达到最大文件数限制时，Redis在accept新连接的时候会立即报错”Max open files”，无法成功获取该连接，此时，listen socket是持续可读的状态，事件循环直接把CPU跑满。这种现象还可能与没有配置Redis配置文件中的<code>timeout</code>参数有关，不设<code>timeout</code>参数，Redis不会主动关闭僵尸连接，导致连接数越来越高，容易达到限制。<br>解决方法：1）<code>config set timeout xx</code>修改keeplive时间；2）<code>client kill ip:port</code>杀死一部分僵尸连接；3）修改系统参数，增加单进程能打开的最大文件数。<br>这一点Memcache和Redis就不同，当Memcache出现这种情况的时候，会立即调用<code>listen(fd, 0)</code>，将accept队列的backlog队列设为0，这样就会直接拒绝掉后续的连接，减轻系统的负载。</p>
<h2 id="2-慢查询"><a href="#2-慢查询" class="headerlink" title="2. 慢查询"></a>2. 慢查询</h2><p>Redis是单线程的，当有单个操作需要处理较长时间，就会阻塞后续的连接，可能是涉及大容量集合相关的CPU密集的运算也可能是操作的value值较大，Redis本身提供慢查询日志的查看，可以查看慢查询日志来分析是哪些操作占用CPU时间过长。<br>解决方法：限制key和value的大小，如果可以，不要让Redis执行过于复杂的计算，而是获取数据后，提交给异步执行的服务端计算。</p>
<h2 id="3-Redis持久化机制"><a href="#3-Redis持久化机制" class="headerlink" title="3. Redis持久化机制"></a>3. Redis持久化机制</h2><p>Redis在fork出子进程做RDB或者AOF的保存时，如果数据量较大，这个不光是CPU可能飙升，内存使用也会有危险。<br>解决方法：1）主库可以不做持久化，将持久化交给从库来做；2）主从复制采用链表式结构，而不是在主库上增加较多从库，这样主库的复制压力较大。3）使用RDB快照的方式做持久化的话，Redis的配置一定要设maxmemory，该值不要超过机器内存的1/2，防止出现内存占满，使用swap而性能下降。</p>
<h2 id="4-Redis的键删除机制"><a href="#4-Redis的键删除机制" class="headerlink" title="4. Redis的键删除机制"></a>4. Redis的键删除机制</h2><p>Redis的键删除机制主要是两种：1）内存不足时的键删除（释放内存）；2）过期键的删除。<br>1）内存不足时：Redis在执行命令时，都会执行一个<code>freeMmoryIfNeeded</code>的函数，当已用内存超过最大内存时（server.maxmemory），根据配置文件中的以下六种策略之一进行删除：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰</div><div class="line">volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰</div><div class="line">volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰</div><div class="line">allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰</div><div class="line">allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰</div><div class="line">no-enviction（驱逐）：禁止驱逐数据</div></pre></td></tr></table></figure></p>
<p>这就是尽量要设置maxmemory和键删除策略的原因，这种策略是保证Redis不会无限制的使用内存。<br>2）过期键的清除策略：这种策略主要有两种方法，首先是惰性删除，所有读写数据库的Redis命令在执行之前都会调用expireIfNeeded函数对输入键进行检查。如果过期就删除，如果没过期就正常访问。其次是定期删除，每当周期性函数serverCron执行时，会调用activeExpireCycle进行主动的过期键删除。具体方法是在规定的时间内，多次从expires中随机挑一个键，检查它是否过期，如果过期则删除。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-连接数过高&quot;&gt;&lt;a href=&quot;#1-连接数过高&quot; class=&quot;headerlink&quot; title=&quot;1. 连接数过高&quot;&gt;&lt;/a&gt;1. 连接数过高&lt;/h2&gt;&lt;p&gt;Redis连接数过高，且没有修改进程能打开的最大文件数，当达到最大文件数限制时，Redis在ac
    
    </summary>
    
    
      <category term="Redis" scheme="https://cyrusin.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>ioloop的blocking_signal</title>
    <link href="https://cyrusin.github.io/2016/08/04/ioloop-blocking-signal-20160804/"/>
    <id>https://cyrusin.github.io/2016/08/04/ioloop-blocking-signal-20160804/</id>
    <published>2016-08-04T08:07:10.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>这里的blocking signal里的blocking并不是传统意义上的针对IO的blocking, 尽管这可能是引起ioloop阻塞的一个原因之一。在这里，blocking指的是ioloop在epoll返回之后开始依次处理各监听文件句柄上的IO事件时，直到下一次进入epoll调用的这段时间的ioloop的状态。我们知道Tornado是单线程的，在处理完某次epoll调用返回的读写就绪事件之前，Tornado无法启动下次epoll监听，所以这段时间理论上是越短越好，这样，ioloop可以充分及时的获取就绪文件句柄，不会影响整体IO性能。然而在实际的使用过程中，难免会出现某次处理时间过长，从而导致ioloop的blocking时间过长的现象。此时，假如有大量连接到达或者有多个IO事件等待处理，ioloop是不能及时获取的，进而会影响性能。</p>
<p>Tornado在ioloop中给我们提供了针对blocking_signal的处理方法，通过singal实现软中断，当ioloop的blocking时间达到阈值时，允许我们注册信号处理程序，我们可以借此实现一些额外的功能，比如当blocking超过一定时间时，log出当前调用栈，这样后续可以分析出到底哪块儿调用耗费了太多时间。</p>
<p>使用这个功能时，可以在ioloop启动之前针对ioloop的实例调用<code>set_blocking_signal_threshold</code>方法，这个方法的实现是:</p>
<pre><code>def set_blocking_signal_threshold(self, seconds, action):
    if not hasattr(signal, &quot;setitimer&quot;):
        gen_log.error(&quot;set_blocking_signal_threshold requires a signal module &quot;
                      &quot;with the setitimer method&quot;)
        return
    self._blocking_signal_threshold = seconds
    if seconds is not None:
        signal.signal(signal.SIGALRM,
                      action if action is not None else signal.SIG_DFL)
</code></pre><p>从代码可以看出，主要是注册了针对signal.SIGALRM信号的处理程序，这个程序有我们自己来定义。以log出当前调用栈为例:</p>
<pre><code>def log_stack(self, signal, frame):
    &quot;&quot;&quot;Signal handler to log the stack trace of the current thread.

    For use with `set_blocking_signal_threshold`.
    &quot;&quot;&quot;
    gen_log.warning(&apos;IOLoop blocked for %f seconds in\n%s&apos;,
                    self._blocking_signal_threshold,
                    &apos;&apos;.join(traceback.format_stack(frame)))
</code></pre><p>即标准的信号处理程序的定义方法，log出当前调用栈的话，就直接记日志就行。</p>
<p>Tornado是如何在ioloop中触发signal的，看ioloop的<code>start</code>方法的实现:</p>
<pre><code>def start(self):
   ...
   try:
        while True:
        ...
        if self._blocking_signal_threshold is not None:
        # clear alarm so it doesn&apos;t fire while poll is waiting for events.
        signal.setitimer(signal.ITIMER_REAL, 0, 0) # 1
        try:
            event_pairs = self._impl.poll(poll_timeout)
        except Exception as e: 
            ...
        if self._blocking_signal_threshold is not None:
            signal.setitimer(signal.ITIMER_REAL, self._blocking_signal_threshold, 0) # 2
        #处理event_pairs
    except:
        ....
</code></pre><p>ioloop的<code>start</code>方法的大致框架如上所示，可以看出其实现技巧和注意点是在调用epoll之前，务必将timer清除，防止signal的软中断错误的打断epoll对IO事件的监听，然后在epoll返回之后，假如之前有调用过<code>set_blocking_signal_threshold</code>方法的话，<code>_blocking_signal_threshold</code>不为None，则会设置<code>signal.setitimer</code>这个定时器。而这个<code>signal.ITIMER_REAL</code>对应的定时器则会在<code>self._blocking_signal_threshold</code>时间(假如IO事件的处理到这个阈值还没结束)之后发出<code>signal.SIGALRM</code>信号。我们注册的signal.SIGALRM的信号处理程序就会被调用。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这里的blocking signal里的blocking并不是传统意义上的针对IO的blocking, 尽管这可能是引起ioloop阻塞的一个原因之一。在这里，blocking指的是ioloop在epoll返回之后开始依次处理各监听文件句柄上的IO事件时，直到下一次进入ep
    
    </summary>
    
      <category term="Tornado" scheme="https://cyrusin.github.io/categories/Tornado/"/>
    
    
      <category term="Tornado" scheme="https://cyrusin.github.io/tags/Tornado/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>greenlet上下文切换的原理</title>
    <link href="https://cyrusin.github.io/2016/07/28/greenlet-20150728/"/>
    <id>https://cyrusin.github.io/2016/07/28/greenlet-20150728/</id>
    <published>2016-07-28T07:27:27.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>greenlet是Python众多协程实现技术中的一种，eventlet是基于greenlet实现的。而eventlet和libev又是gevent的核心。greenlet的上下文切换清晰易懂，可以结合IO事件循环构建出一些高效的事件处理逻辑。不同于<code>yield</code>类型的上下文切换，greenlet的上下文切换从表现形式上看更纯粹，可以直接<code>switch</code>到另一个greenlet，不用管目标greenlet是否已经在运行，不同greenlet之间处于完全对等的状态，可以相互<code>switch</code>。基于<code>yield</code>实现的协程往往只能切换回自己的直接或间接调用者，要想在嵌套的调用中切换出去是比较麻烦的。本质上是因为<code>yield</code>只能保留栈顶的帧，Python3对此有改进，可以通过<code>yield from</code>嵌套的挂起内层过程调用，但依然不能任意的切换到其他上下文。而greenlet就可以，只要一个过程被封装进一个greenlet，可以认为这个greenlet就成了一个可以随时挂起和恢复的实体。当然这种灵活性的代价是代码的逻辑会变得混乱，debug会更难，不过如果适当的使用greenlet，却可以收到很好的效果，比如之前介绍过的Motor。</p>
<p>协程是用户态下的上下文切换技术，是对线程时间片的再切分。所谓上下文，一般是指一个子过程调用，比如一个函数。另一方面，我们知道，Python虚拟机的原理是通过栈帧对象PyFrameObject抽象出运行时（栈，指令，符号表，常量表等），通过执行<code>PyEval_EvalFrameEx</code>这个C级别的函数来逐个解析字节码指令。也就是说可调用对象都是通过<code>PyEval_EvalFrameEx</code>来执行自己的PyFrameObject的，而按照调用的先后顺序，当前PyFrameObject的<code>f_back</code>指针会指向上一个PyFrameObject，这样，某一个时刻，当前线程的栈帧对象按调用的先后顺序形成了一个链表， 线程的top_frame属性正好是链表的表头（栈顶），这就是当前线程正在运行的帧。从这种状态可以看出，对于Python而言，切换当前栈顶的帧是容易的，只要保留栈顶的PyFrameObject，回退到栈顶下的一帧就行，这也是<code>yield</code>的基本原理。但问题是，Python的栈帧对象本身从一开始并不是为协程设计的，所以栈帧与栈帧之间的这种执行的先后顺序（其实可以理解为执行栈）语言本身却没有提供恢复和挂起的机制。这就要求假如你想任意切换上下文的话，必须实现一个机制，可以保存一个执行栈。</p>
<p>通过上面的分析，可以看到，要想在Python中在两个子过程中作任意的挂起和恢复的话，需要做到两点：</p>
<ol>
<li>保存当前子过程的执行栈。</li>
<li>恢复目标子过程的执行栈，并恢复栈顶的状态继续执行。</li>
</ol>
<p>greenlet之所以可以在任意两个greenlet之间作切换，就是因为其实现了上述的两点。其总共加起来2000多行C代码，其中内联了一小部分但确实相当关键的汇编代码，看懂greenlet的代码至少要把C语言的过程调用原理、汇编、进程的堆栈、Python的虚拟机执行原理等弄清楚，如果有一个不懂，就不用看源码了，能用起来就好。毕竟有些部分很绕，光看源码分析也不一定能完全消化吸收，有些东西也很难用文字表达，只可意会，不可言传。</p>
<p>greenlet的基本原理简单说起来就是:</p>
<ol>
<li>将一个子过程封装进一个greenlet里，而一个greenlet代表了一段C的栈内存。</li>
<li>在greenlet里执行Python的子过程(通常是个函数)，当要切换出去的时候，保存当前greenlet的栈内存，方法是memcpy到堆上，也就是说每一个greenlet可能都需要在堆上保存上下文，挂起的时候就把栈内存memcpy到堆上，恢复的时候再把堆上的上下文（运行的时候栈内存的内容）拷贝到栈上，然后释放堆上的内存。</li>
<li>恢复栈顶只需要将当前线程的top_frame修改为恢复的greenlet的top_frame就行。</li>
</ol>
<p>greenlet的基本原则：</p>
<ol>
<li>除了main greenlet之外，任意一个greenlet都有唯一一个父greenlet。</li>
<li>假如当前greenlet执行完毕，回到自己的父greenlet即可。</li>
<li>可以通过给switch方法的参数来在不同greenlet之间传递数据。</li>
</ol>
<p>greenlet实现的关键是先切换C函数的栈，而切换和恢复C的栈需要将%ebp（函数栈底）、%esp（函数栈顶）等寄存器的值保存到本地变量，而恢复的时候就可以通过从堆上拷贝的内存，来恢复寄存器的值。从而达到恢复上下文的目的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;greenlet是Python众多协程实现技术中的一种，eventlet是基于greenlet实现的。而eventlet和libev又是gevent的核心。greenlet的上下文切换清晰易懂，可以结合IO事件循环构建出一些高效的事件处理逻辑。不同于&lt;code&gt;yield&lt;
    
    </summary>
    
      <category term="协程" scheme="https://cyrusin.github.io/categories/%E5%8D%8F%E7%A8%8B/"/>
    
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
      <category term="C" scheme="https://cyrusin.github.io/tags/C/"/>
    
      <category term="协程" scheme="https://cyrusin.github.io/tags/%E5%8D%8F%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>理解Motor: greenlet加持的异步非阻塞MongoDB驱动分析</title>
    <link href="https://cyrusin.github.io/2016/06/06/motor20160606/"/>
    <id>https://cyrusin.github.io/2016/06/06/motor20160606/</id>
    <published>2016-06-06T06:43:59.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>最不喜欢在Tornado中使用任何同步阻塞型的东西，不想让ioloop阻塞在某个IO调用上，因为单线程的东西任何阻塞都是代价很高的，除非你的数据库被优化的性能很好，速度很快。除了之前的线程池之外，直接使用异步库也是不错的选择，Motor就是Tornado里可以用的很好的异步库，它兼容Tornado的<code>gen.coroutine</code>式的异步调用形式，主要使用了<code>greenlet</code>来巧妙的封装PyMongo的同步API, 把底层的<code>socket</code>IO进行了异步化的处理，化同步为异步。</p>
<p>从使用的例子来分析Motor是如何把PyMongo的API异步化的:</p>
<pre><code>client = motor.MotorClient(...)
db = client[&apos;testDB&apos;]
...
class SomeHandler(web.RequestHandler):
    @gen.coroutine
    def get(self):
        ...
        doc = yield db.test_collection.find_one({&apos;i&apos;: {&apos;$lt&apos;: 1}})
        ...
</code></pre><p>以上只是简单的示例使用Motor的大概流程，当我们通过MotorClient生成client的时候，Motor将自己实现的<code>MotorPool</code>作为<code>pool_class</code>传给PyMongo的<code>MongoClient</code>的构造函数，这样连接池就被替换成了Motor自己的连接池，而<code>MotorPool</code>在通信过程中则使用了Motor自己封装的异步socket。</p>
<p>Motor除了在底层上替换原有的通信过程，另外对PyMongo的API做了异步化的处理，使之兼容Tornado的异步调用过程。和PyMongo使用的不同就是像代码里展示的那样，类似Tornado里其他的异步调用一样，通过yield出一个Future来暂时挂起上下文，当Future被<code>set_result</code>的时候，在ioloop上注册callback来恢复上下文。也就是说:<code>doc = yield db.test_collection.find_one({&#39;i&#39;: {&#39;$lt&#39;: 1}})</code>这句并不会阻塞ioloop，而是暂时挂起了。Motor做到这点主要是利用了greenlet来封装原始的PyMongo的同步API。</p>
<p>看一下类似<code>find_one</code>这样的PyMongo的操作API(其余还有<code>find</code>、<code>insert</code>等等)是怎样被异步化的:</p>
<pre><code>def asynchronize(motor_class, framework, sync_method, has_write_concern, doc=None):
    @functools.wraps(sync_method)
    def method(self, *args, **kwargs):
        check_deprecated_kwargs(kwargs)
        loop = self.get_io_loop() #比如: Tornado的IOLoop
        callback = kwargs.pop(&apos;callback&apos;, None)

        if callback:
            if not callable(callback):
                raise callback_type_error
            future = None
        else:
            #在Tornado的gen.coroutine中yield出来
            future = framework.get_future(self.get_io_loop())

        #call_method是实际的对PyMongo的sync_method的封装,
        #这个方法的调用将在一个子greenlet中进行
        def call_method():
            try:
                #sync_method 是实际的同步的PyMongo的同步调用
                #也许会疑惑：即使在子greenlent中运行，不依然会阻塞吗？
                #稍后会解决这个疑惑
                result = sync_method(self.delegate, *args, **kwargs)
                #给yield出去的future赋予结果，可以恢复外部的上下文(gen.coroutine)
                if callback:
                    # Schedule callback(result, None) on main greenlet.
                    framework.call_soon(
                        loop,
                        functools.partial(callback, result, None))
                else:
                    # Schedule future to be resolved on main greenlet.
                    framework.call_soon(
                        loop,
                        functools.partial(future.set_result, result))
            except Exception as e:
                if callback:
                    framework.call_soon(
                        loop,
                        functools.partial(callback, None, e))
                else:
                    # TODO: we lost Tornado&apos;s set_exc_info. Frameworkify this.
                    framework.call_soon(
                        loop,
                        functools.partial(future.set_exception, e))

        #在一个子greenlet中运行这个同步的方法封装
        #注意: 这个greenlet是我们在当前方法中生成的，也就是说我们当前的方法所处的
        #greenlet是我们生成的greenlet的父greenlet，这个在后续的上下文切换中非常重要
        greenlet.greenlet(call_method).switch() #切换到子greenlet执行
        #返回future
        return future
    ...
    return method
</code></pre><p>以上代码会封装PyMongo的同步操作，使之可以在Tornado的<code>gen.coutoine</code>中yield出来，参数<code>sync_method</code>就是原始的PyMongo的同步调用。通过注释理解这种封装的方法，原理其实就是：把同步调用放到一个子greenlet中去执行，当子greenlet阻塞时，切换上下文回父greenlet执行，也就是yield出future，这样就不会阻塞主ioloop了。最关键的问题开始：1)什么时候切换回父greenlet? 2)当数据到达，阻塞解除时，再怎么回到子greenlet继续执行?</p>
<p>我们说过，Motor用自己的<code>MotorPool</code>取代了PyMongo的连接池，回看下之前代码，比如我们调用<code>find_one</code>，我们会在子greenlet中执行，而<code>find_one</code>里则会走PyMongo的同步的那一套逻辑，但是当代码走到最底层的实际的socket的IO时，就走Motor的socket的IO了，因为连接池已经被Motor移花接木，换成了自己的那一套。</p>
<p>当sync_method里的同步逻辑走到socket的IO时，原有的<code>socket.recv</code>被Motor用异步的<code>recv</code>代替了:</p>
<pre><code>@tornado_motor_sock_method
def recv(self, num_bytes):
    #借助了Tornado的IOStream的异步读写封装
    #IOStream的异步读写不会阻塞，而是一种事件驱动的异步非阻塞IO
    future = stream_method(self.stream, &apos;read_bytes&apos;, num_bytes)
    try:
        if self.timeout_td:
            result = yield _Wait(future, self.io_loop, self.timeout_td, timeout_exc)
        else:
            result = yield future
    except IOError as e:
        raise socket.error(str(e))
    raise gen.Return(result)
</code></pre><p>从以上针对socket的recv方法看出，通过Tornado的异步读写封装（IOStream），实现了异步的socket，目前关键的就是<code>tornado_motor_sock_method</code>了，很明显，这个装饰器将完成greenlet上下文的父子切换，解决我们之前在<code>sync_method</code>调用时的疑惑。</p>
<p>看一下<code>tornado_motor_sock_method</code>这个装饰器的实现：</p>
<pre><code>def tornado_motor_sock_method(method):
    coro = gen.coroutine(method)

    @functools.wraps(method)
    def wrapped(self, *args, **kwargs):
        #当前greenlet是一个子greenlet
        child_gr = greenlet.getcurrent()
        #获取当前greenlet的父greenlet，即之前代码提到过的asynchronize所在的greenlet
        main = child_gr.parent

        def callback(future):
            if future.exc_info():
                child_gr.throw(*future.exc_info())
            elif future.exception():
                child_gr.throw(future.exception())
            else:
                #当future的结果到达，切换回挂起的子greenlet
                child_gr.switch(future.result())

        #保证callback在当前greenlet的父greenlet中运行
        self.io_loop.add_future(coro(self, *args, **kwargs), callback)
        #return这句会暂时挂起当前greenlet，将控制权切换回父greenlet，
        #在上面的callback执行时，才会切换回当前greenlet，return语句返回
        return main.switch()
    return wrapped
</code></pre><p>以<code>find_one</code>调用为例，<code>find_one</code>的一系列调用都在一个greenlet中进行，比如最终走到了<code>resut = socket.recv(...)</code>这句，通过以上的代码可以发现，这句会暂时被挂起这个greenlet，并把控制权暂时切换回当前greenlet的父greenlet，也就是asynchronize方法所在的主greenlet，而回看之前的asynchronize的代码，发现切换回去之后，asynchronize调用立即返回future，也就是<code>doc = yield db.test_collection.find_one(...)</code>会yield出这个future，这样，ioloop没有被阻塞。而当socket上数据到达时，我们会通过在这句<code>self.io_loop.add_future(coro(self, *args, **kwargs), callback)</code>里添加的callback切换回挂起的子greenlet，也就是<code>return main.switch()</code>这句会返回，<code>result = socket.recv(...)</code>这句就恢复执行，这样，刚才挂起的greenlet就继续往下执行了，最终执行到asynchronize里的<code>call_method</code>里的<code>result = sync_method(self.delegate, *args, **kwargs)</code>这句返回，再回顾之前的asynchronize的逻辑，这句返回后，通过<code>framework.call_soon(loop, functools.partial(future.set_result, result))</code>这句，yield出去的future被<code>set_result</code>，这样，<code>doc = yield db.test_collection.find_one(...)</code>挂起的上下文稍后也会恢复执行了。</p>
<p>以上就是Motor利用Tornado的ioloop和iostream以及greenlet来封装PyMongo并将其异步化的过程。</p>
<p>另附, Motor作者在自己博客上的介绍文章:<br><a href="https://emptysqua.re/blog/introducing-motor-an-asynchronous-mongodb-driver-for-python-and-tornado/" target="_blank" rel="external">Introducing Motor, an asynchronous MongoDB driver for Python and Tornado</a><br><a href="https://emptysqua.re/blog/motor-internals-how-i-asynchronized-a-synchronous-library/" target="_blank" rel="external">Motor Internals: How I Asynchronized a Synchronous Library</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最不喜欢在Tornado中使用任何同步阻塞型的东西，不想让ioloop阻塞在某个IO调用上，因为单线程的东西任何阻塞都是代价很高的，除非你的数据库被优化的性能很好，速度很快。除了之前的线程池之外，直接使用异步库也是不错的选择，Motor就是Tornado里可以用的很好的异步
    
    </summary>
    
      <category term="技术" scheme="https://cyrusin.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
      <category term="MongoDB" scheme="https://cyrusin.github.io/tags/MongoDB/"/>
    
      <category term="Motor" scheme="https://cyrusin.github.io/tags/Motor/"/>
    
  </entry>
  
  <entry>
    <title>从Ctrl-C看Python多线程的信号处理</title>
    <link href="https://cyrusin.github.io/2016/05/25/python20160525/"/>
    <id>https://cyrusin.github.io/2016/05/25/python20160525/</id>
    <published>2016-05-25T08:04:24.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>也许你会偶然发现Python的多线程程序使用<code>Ctrl-C</code>杀不掉，必须拿到pid用<code>kill -9</code>才能干掉，研究这个问题的原因可以使得对Python多线程的信号处理及线程的退出机制有更好的理解。</p>
<p>假如有一个Python写成的用多线程模拟<code>生产者-消费者</code>的程序，代码如下:</p>
<pre><code>class Producer(threading.Thread):
    def run(self):
        global count
        while True:
            if cond.acquire():
                if count &gt; 1000:
                    cond.wait()
                else:
                    count = count + 100
                    print &quot;%s produce 100, count=%s&quot; % (self.name, count)
                    cond.notify()
                cond.release()

class Consumer(threading.Thread):
    def run(self):
        global count
        while True:
            if cond.acquire():
                if count &lt; 100:
                    cond.wait()
                else:
                    count = count - 100
                    print &quot;%s consume 100, count=%s&quot; % (self.name, count)
                    cond.notify()
                cond.release()
count = 500
cond = threading.Condition()

for i in xrange(4):
    p = Producer()
    producers.append(p)
    p.start()

for j in xrange(2):
    c = Consumer()
    consumers.append(c)
    c.start()
</code></pre><p>执行以上程序，直接<code>Ctrl-C</code>是杀不掉的，虽然<code>Ctrl-C</code>是向程序发送<code>SIGINT</code>信号，但在这里，这个<code>Ctrl-C</code>被无视了。从常识上讲，我们是希望程序可以响应<code>Ctrl-C</code>并立即停止的。</p>
<h1 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h1><p>Python多线程中，<strong>只有主线程可以响应并处理信号</strong>，然而我们看到，主线程在生成了各子线程之后，就歇了，我们知道，主线程不能挂的，那么主线程干嘛去了？答案是主线程在准备退出时，阻塞等待所有非daemon线程去了，看源码：</p>
<pre><code>class _MainThread(Thread):

    def __init__(self):
        Thread.__init__(self, name=&quot;MainThread&quot;)
        self._Thread__started.set()
        self._set_ident()
        with _active_limbo_lock:
            _active[_get_ident()] = self

    def _set_daemon(self):
        return False

    def _exitfunc(self):
        self._Thread__stop()
        t = _pickSomeNonDaemonThread()
        if t:
            if __debug__:
                self._note(&quot;%s: waiting for other threads&quot;, self)
        while t:
            t.join() #阻塞等待该子线程结束
            t = _pickSomeNonDaemonThread()
        if __debug__:
            self._note(&quot;%s: exiting&quot;, self)
        self._Thread__delete()

    def _pickSomeNonDaemonThread():
        for t in enumerate():
            if not t.daemon and t.is_alive(): #只管非daemon的活跃线程
                return t
        return None
</code></pre><p>很明显，主线程已经阻塞等待在了join()上，所以我们的信号没有被主线程及时处理，同时，从以上源码可以看出，<strong>Python主线程退出时会等待所有活跃的非daemon线程退出</strong>。</p>
<p>如果我们把子线程设为<code>daemon=True</code>，主线程则不会等待子线程，执行完之后会直接退出，所以我们还是得在主线程中显式的对各子线程join()，又回到了以上的情况而已，不能解决问题。</p>
<h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><p>从以上的分析看出：</p>
<p>首先，我们需要各子线程均为<code>daemon=True</code>，保证我们的主线程退出的时候，可以直接退出。</p>
<p>其次，主线程不能join()等待子线程，否则会阻塞而无法及时响应信号，那么主线程干点什么好呢？</p>
<p>方案就是：</p>
<p>在主线程注册信号处理函数，并监听<code>Ctrl-C</code>的信号<code>SIGINT</code>和<code>kill</code>的默认信号<code>SIGTERM</code>，设置一全局变量，各子线程都会在循环执行任务时检测该全局变量，而主线程也会循环检测该全局变量。在信号处理函数中，收到信号时，修改该全局变量的值，通过该全局变量通知子线程退出，而主线程在各子线程退出后，也退出。</p>
<pre><code>class Producer(threading.Thread):
    def run(self):
        global count
        global is_exit
        while True:
            if cond.acquire():
                if is_exit: #每次获取锁之后，先检查全局状态变量
                    cond.notifyAll() #退出前必须唤醒其他所有线程
                    cond.release() #退出前必须释放锁
                    break
                if count &gt; 1000:
                    cond.wait()
                else:
                    count = count + 100
                    print &quot;%s produce 100, count=%s&quot; % (self.name, count)
                    cond.notify()
                cond.release()
class Consumer(threading.Thread):
    def run(self):
        global count
        global is_exit
        while True:
            if cond.acquire():
                if is_exit:
                    cond.notifyAll()
                    cond.release()
                    break
                if count &lt; 100:
                    cond.wait()
                else:
                    count = count - 100
                    print &quot;%s consume 100, count=%s&quot; % (self.name, count)
                    cond.notify()
                cond.release()
count = 500
cond = threading.Condition()
is_exit = False #全局变量
def signal_handler(signum, frame): #信号处理函数
    global is_exit
    is_exit = True #主线程信号处理函数修改全局变量，提示子线程退出
    print &quot;Get signal, set is_exit = True&quot;
def test():
    producers = []
    consumers = []
    for i in xrange(4):
        p = Producer()
        producers.append(p)
        p.setDaemon(True) #子线程daemon
        p.start()
    for j in xrange(2):
        c = Consumer()
        consumers.append(c)
        c.setDaemon(True) #子线程daemon
        c.start()
    while 1:
        alive = False
        for t in itertools.chain(producers, consumers): #循环检查所有子线程
            alive = alive or t.isAlive() #保证所有子线程退出
        if not alive:
            break
if __name__ == &quot;__main__&quot;:
    signal.signal(signal.SIGINT, signal_handler) #注册信号处理函数
    signal.signal(signal.SIGTERM, signal_handler) #注册信号处理函数
    test()
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;也许你会偶然发现Python的多线程程序使用&lt;code&gt;Ctrl-C&lt;/code&gt;杀不掉，必须拿到pid用&lt;code&gt;kill -9&lt;/code&gt;才能干掉，研究这个问题的原因可以使得对Python多线程的信号处理及线程的退出机制有更好的理解。&lt;/p&gt;
&lt;p&gt;假如有一个Pyt
    
    </summary>
    
      <category term="Python" scheme="https://cyrusin.github.io/categories/Python/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python多线程join()的实现细节</title>
    <link href="https://cyrusin.github.io/2016/04/28/python-join-implementation/"/>
    <id>https://cyrusin.github.io/2016/04/28/python-join-implementation/</id>
    <published>2016-04-28T06:44:24.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>写Python多线程都知道当前线程调用<code>a.join()</code>后，会阻塞直到线程a运行结束，看了一下<code>threading</code>模块的源码,<br>了解了一下实现的原理。</p>
<p>每一个新开启的线程内部都维护着一个<code>Condition</code>类型的条件变量，对线程a进行<code>join()</code>，其实是<code>wait()</code>在线程a<br>内部的条件变量上，当线程a执行结束时，会通过<code>notify_all()</code>通知所有<code>join()</code>的线程，则阻塞的线程被唤醒，恢复执行。</p>
<p>以下是源码:</p>
<pre><code>self.__block = Condition(Lock()) #线程内部维护的Contition变量
def __stop(self):
    if not hasattr(self, &apos;_Thread__block&apos;):
        return
    self.__block.acquire()
    self.__stopped = True
    self.__block.notify_all() #唤醒所有join()的线程
    self.__block.release()
def join(self, timeout=None): #可以添加timeout
    ...
    self.__block.acquire()
    try:
        if timeout is None:
            while not self.__stopped:
                self.__block.wait() #等待线程运行结束
    else:
        deadline = _time() + timeout
        while not self.__stopped:
            delay = deadline - _time()
            if delay &lt;= 0:
                break
            self.__block.wait(delay)
    finally:
        self.__block.release()
</code></pre><p>由于是<code>Condition</code>，所以线程可以被<code>join()</code>多次，最终都会由<code>notify_all()</code>唤醒。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;写Python多线程都知道当前线程调用&lt;code&gt;a.join()&lt;/code&gt;后，会阻塞直到线程a运行结束，看了一下&lt;code&gt;threading&lt;/code&gt;模块的源码,&lt;br&gt;了解了一下实现的原理。&lt;/p&gt;
&lt;p&gt;每一个新开启的线程内部都维护着一个&lt;code&gt;Cond
    
    </summary>
    
      <category term="Python" scheme="https://cyrusin.github.io/categories/Python/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>GIL的实现细节</title>
    <link href="https://cyrusin.github.io/2016/04/27/python-gil-implementaion/"/>
    <id>https://cyrusin.github.io/2016/04/27/python-gil-implementaion/</id>
    <published>2016-04-27T07:01:23.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GIL"><a href="#GIL" class="headerlink" title="GIL"></a>GIL</h1><p>熟悉Python的人对GIL这货可定都不陌生, <strong>全局解释器锁(Global Interpreter Lock)</strong>简称<strong>GIL</strong>, 这货是<br>Python多线程的核心机制。由于Python的线程实际是操作系统的原生线程,多个线程同时执行同一段字节码可能<br>会导致很多问题(比如: 内存管理的引用计数需要线程安全机制的保护),于是使用GIL这把大锁锁住其他线程,保<br>证同一时刻只有一个线程可以解释执行字节码。关于GIL的更多分析,<br>可以看<a href="http://www.dabeaz.com/GIL/" target="_blank" rel="external">David Beazley大神的研究</a>。本文主要分析下<strong>CPython</strong>的GIL在<strong>Linux</strong>上<br>基于<strong>pthread</strong>的实现细节,看完这些源码后能够对GIL有更深入的理解。</p>
<h1 id="GIL的定义"><a href="#GIL的定义" class="headerlink" title="GIL的定义"></a>GIL的定义</h1><p>有人可能会想,从GIL的介绍来看不就是一个互斥锁吗?有什么好分析的,直接pthread_mutex_lock()/pthread_mutex_unlock()<br>不就行了,其实不然,pthread原生的mutex lock是不足以实现我们的要求的,GIL的目的是让多个线程按一定条件并发执行<br>而非简单互斥,CPython源码的注释也说了:</p>
<pre><code>/* A pthread mutex isn&apos;t sufficient to model the Python lock type
 * because, according to Draft 5 of the docs (P1003.4a/D5), both of the
 * following are undefined:
 *  -&gt; a thread tries to lock a mutex it already has locked
 *  -&gt; a thread tries to unlock a mutex locked by a different thread
 * pthread mutexes are designed for serializing threads over short pieces
 * of code anyway, so wouldn&apos;t be an appropriate implementation of
 * Python&apos;s locks regardless.
 *
 * The pthread_lock struct implements a Python lock as a &quot;locked?&quot; bit
 * and a &lt;condition, mutex&gt; pair.  In general, if the bit can be acquired
 * instantly, it is, else the pair is used to block the thread until the
 * bit is cleared.     9 May 1994 tim@ksr.com
 */
来自CPython源码(Python/thread_pthread.h), Python版本: 2.7.8
</code></pre><p>从以上注释来看, 由于Linux上的POSIX线程实现有些未定义行为, 并且mutex lock只适用于同步线程对于小段临界<br>区代码的访问，所以并不适合作为GIL的原生实现。而Python的GIL实际是一个<code>&lt;condition, mutex&gt;</code>对, 并用这个<br>条件变量和互斥锁来保护一个<code>locked</code>状态变量。下面来看GIL的真面目:</p>
<pre><code>typedef struct {
    char             locked; /* 0=unlocked, 1=locked */
    /* a &lt;cond, mutex&gt; pair to handle an acquire of a locked lock */
    pthread_cond_t   lock_released;
    pthread_mutex_t  mut;
} pthread_lock;
</code></pre><p>以上这个结构体就是GIL的定义, 可以看出, <code>locked</code>用来指示是否上锁, <code>1</code>表示已有线程上锁, <code>0</code>表示锁空闲。<br>而<code>lock_released</code>和<code>mutex</code>来同步对<code>locked</code>的访问。</p>
<h1 id="GIL的获取与释放"><a href="#GIL的获取与释放" class="headerlink" title="GIL的获取与释放"></a>GIL的获取与释放</h1><p>从GIL的定义来看,线程对GIL的操作本质上就是通过修改<code>locked</code>状态变量来获取或释放GIL。所以主要的操作有两个:</p>
<ol>
<li>PyThread_acquire_lock()</li>
<li>PyThread_release_lock()</li>
</ol>
<p>下面分别看看起具体实现:</p>
<pre><code>/*获取GIL*/
int  PyThread_acquire_lock(PyThread_type_lock lock, int waitflag) 
{
    int success;
    pthread_lock *thelock = (pthread_lock *)lock;
    int status, error = 0;
    status = pthread_mutex_lock( &amp;thelock-&gt;mut ); /*先获取mutex, 获得操作locked变量的权限*/
    success = thelock-&gt;locked == 0;
    if ( !success &amp;&amp; waitflag ) { /*已有线程上锁,*/
        while ( thelock-&gt;locked ) {
            /*通过pthread_cond_wait等待持有锁的线程释放锁*/
            status = pthread_cond_wait(&amp;thelock-&gt;lock_released,
                                       &amp;thelock-&gt;mut);
        }
        success = 1;
    }
    if (success) thelock-&gt;locked = 1; /*当前线程上锁*/
    status = pthread_mutex_unlock( &amp;thelock-&gt;mut ); /*解锁mutex, 让其他线程有机会进入临界区等待上锁*/
    if (error) success = 0;
    return success; 
}
</code></pre><p>以上就是获取GIL的实现,可以看到,线程在其他线程已经获取GIL的时候,需要通过pthread_cond_wait()等待获取GIL的线程释放GIL。</p>
<pre><code>/*释放GIL*/
void PyThread_release_lock(PyThread_type_lock lock)
{
    pthread_lock *thelock = (pthread_lock *)lock;
    int status, error = 0;
    status = pthread_mutex_lock( &amp;thelock-&gt;mut ); /*通过mutex获取操作locked变量的权限*/
    thelock-&gt;locked = 0; /*实际的释放GIL的操作, 就是将locked置为0*/
    status = pthread_mutex_unlock( &amp;thelock-&gt;mut ); /*解锁mutex*/
    status = pthread_cond_signal( &amp;thelock-&gt;lock_released ); /*这步非常关键, 通知其他线程当前线程已经释放GIL*/
}
</code></pre><p>以上就是释放GIL的过程,特别注意最后一步, 通过pthread_cond_signal()通知其他等待(pthread_cond_wait())释放GIL的线程,<br>让这些线程可以获取GIL。</p>
<h1 id="获取与释放GIL的时机"><a href="#获取与释放GIL的时机" class="headerlink" title="获取与释放GIL的时机"></a>获取与释放GIL的时机</h1><p>分析了GIL的获取与释放的实现机制后,我们来看看CPython解释器会在什么时候获取与释放GIL。我们知道,<br>GIL是用来同步多线程使得同一时刻只有一个线程可以解释执行字节码的,显然多线程下,一个线程执行一段<br>时间之后就要释放GIL让其他线程有执行的机会,而且从获取与释放GIL的实现来看,只有持有GIL的线程主动<br>释放GIL,其他线程才有机会获取GIL执行自己的任务。那么到底多长时间之后会释放GIL呢？</p>
<p>首先我们来了解一下CPython解释器解释执行字节码的主循环,这个主循环位于<code>Python/ceval.c</code>文件的<br>函数<code>PyEval_EvalFrameEx()</code>内。这个函数的大体结构是:</p>
<pre><code>{
    ...
    for (;;) { /*解释器主循环*/
        ...
        /*_Py_Ticker是个数字, 比如100, 这段代码使得大概每100次才会进入if内的代码*/
        if (--_Py_Ticker &lt; 0) { 
            ...
            _Py_Ticker = _Py_CheckInterval;
            ...
            if (interpreter_lock) { /*开始进行释放GIL与获取GIL的操作*/
                ...
                PyThread_release_lock(interpreter_lock); /*释放GIL*/
                /*获取GIL, 同时注意到这个操作是紧跟着释放GIL的, 
                这个使得常常当前线程释放了GIL,紧接着又重新获取了GIL*/
                PyThread_acquire_lock(interpreter_lock, 1); 
            }
        }
        ...
        switch (opcode) { /*opcode就是字节码指令*/
            case: ...
        }
    }
}
</code></pre><p>以上代码给出了解释器主循环的代码的整体结构,可以看出,在一个大的循环中逐个解析字节码指令,但是<br>需要注意的是每次循环开始都会检查一下<code>_Py_Ticker</code>的值,这个值可以通过:</p>
<pre><code>python -c &apos;import sys;print sys.getcheckinterval()&apos;
</code></pre><p>查看,默认是100,这个值可以认为是执行的字节码条数的一个计数器,严格上来说应该并不完全等于字节码<br>条数,但是可以这么理解,就是一个当前线程执行了多久的指示器。可以看出,这个数字在执行字节码的<br>过程中是递减的,而每次进入一条新的字节码之前都会检查这个数字,当这个数字小于0的时候,就开始进入if<br>块内部的代码,而内部的代码会释放GIL。所以,这个周期性的计数器小于0就是我们释放GIL的时机之一。</p>
<p>Python释放GIL的时机之一:</p>
<blockquote>
<p>有一个周期性计数的计数器,不断递减,保证Python线程可以在执行一段时间之后释放GIL</p>
</blockquote>
<p>仔细分析就会发现问题,假如在解析执行字节码的过程中当前线程遇到了一个IO操作,却由于等待数据而被阻塞到了<br>该IO操作上,由于只有主动释放GIL,其他线程才有机会运行,那这样显然是白白浪费了时间,从GIL的设计来看,当当前<br>线程阻塞在IO操作上时,此时给其他线程运行的机会并没有什么问题,因为GIL只是用来同步线程执行字节码的,并非<br>一般的互斥共享资源的互斥锁。在阻塞操作之前让出GIL,其他线程可以继续执行,而当前线程可以继续执行阻塞型的<br>操作,当该阻塞型的操作完成之后,再次试图获取GIL,继续执行余下的字节码。Python的设计者已经考虑到了这样的问题:</p>
<pre><code>/* Interface for threads.
A module that plans to do a blocking system call (or something else
 that lasts a long time and doesn&apos;t touch Python data) can allow other
  threads to run as follows:
    ...preparations here...
    Py_BEGIN_ALLOW_THREADS
    ...blocking system call here...
    Py_END_ALLOW_THREADS
    ...interpret result here...
*/
[Python/ceval.h]
</code></pre><p>从以上注释来看,Python允许在执行block型的system call(或者其他不会操作Python Data的调用)之前allow其他<br>线程执行(Py_BEGIN_ALLOW_THREADS),完事儿后,再重新尝试获取GIL(Py_END_ALLOW_THREADS)。<br>Py_BEGIN_ALLOW_THREADS和Py_END_ALLOW_THREADS是两个宏,其完整定义是:</p>
<pre><code>#define Py_BEGIN_ALLOW_THREADS { \
                        PyThreadState *_save; \
                        _save = PyEval_SaveThread();

#define Py_END_ALLOW_THREADS    PyEval_RestoreThread(_save); \
                }
</code></pre><p>而PyEval_SaveThread()和PyEval_RestoreThread()里会分别释放GIL和获取GIL。看一个Python实现中的具体应用的例子:</p>
<pre><code>static PyObject * file_read(...)
{
    ...
    for(;;)
    {
        ...
        FILE_BEGIN_ALLOW_THREADS(f) //释放GIL
        errno = 0;
        chunksize = Py_UniversalNewlineFread(BUF(v) + bytesread,
        buffersize - bytesread, f-&gt;f_fp, (PyObject *)f);
        interrupted = ferror(f-&gt;f_fp) &amp;&amp; errno == EINTR;
        FILE_END_ALLOW_THREADS(f) //获取GIL
        ...
    }
    ...
}
</code></pre><p><code>file_read</code>是<code>f.read()</code>对应的C的实现。可以看到代码里在实际进行读操作之前会尝试释放GIL。</p>
<p>由此可以看出,Python释放GIL的第二个时机:</p>
<blockquote>
<p>在IO操作等可能会引起阻塞的system call之前,可以暂时释放GIL,但在执行完毕后,必须重新获取GIL</p>
</blockquote>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>单核机器上,GIL是比较好的多线程并发的机制,但在多核机器上,GIL使得<strong>计算密集型</strong>Python应用无法充分利用多核;</li>
<li>IO密集型多线程Python应用可以通过GIL获得良好的性能,因为在IO操作时可以暂时释放GIL;</li>
<li>通常情况下,Python通过周期性的check决定是否释放GIL;</li>
<li>在通过C扩展Python时,可以谨慎处理GIL,在不操作Python原生对象的时候,可以尝试Py_BEGIN_ALLOW_THREADS和Py_END_ALLOW_THREADS。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;GIL&quot;&gt;&lt;a href=&quot;#GIL&quot; class=&quot;headerlink&quot; title=&quot;GIL&quot;&gt;&lt;/a&gt;GIL&lt;/h1&gt;&lt;p&gt;熟悉Python的人对GIL这货可定都不陌生, &lt;strong&gt;全局解释器锁(Global Interpreter Lock)&lt;/s
    
    </summary>
    
      <category term="Python" scheme="https://cyrusin.github.io/categories/Python/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Tornado与线程池</title>
    <link href="https://cyrusin.github.io/2016/04/07/tornado-threadpool-20160407/"/>
    <id>https://cyrusin.github.io/2016/04/07/tornado-threadpool-20160407/</id>
    <published>2016-04-07T07:30:29.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>Tornado本身的设计目标是单线程异步非阻塞，要想很好的发挥它的性能最好使用异步IO，并且Tornado本身也提供了异步的<code>AsyncHttpClient</code>的实现，配合<code>gen.coroutine</code>和<code>yield</code>，可以让请求异步执行从而不阻塞当前线程，对于单线程服务器来说，<code>阻塞(blocking)</code>和<code>同步的sleep</code>这种会挂起线程的动作都是服务器的噩梦，因为只有一个线程，所以任何等待都会影响服务器对于其他请求的处理。</p>
<p>异步非阻塞对于第三方IO是http请求的情况还好，毕竟可以使用Tornado提供的异步实现，但是对于有些数据库的IO，则需要异步库的支持，比如针对MongoDB的Motor等。但是第三方异步库的质量也是参差不齐，在实际的工程中，假如没有特别好的异步库的时候，还是只能使用同步的库。</p>
<p>假如只能使用同步阻塞的IO，为了提高性能，可以考虑在Tornado中使用线程池。线程池可以考虑使用<code>concurrent.futures</code>里的<code>ThreadPoolExecutor</code>。</p>
<h1 id="1-在RequestHandler的http请求处理方法-get-post等-中使用线程池"><a href="#1-在RequestHandler的http请求处理方法-get-post等-中使用线程池" class="headerlink" title="1. 在RequestHandler的http请求处理方法(get/post等)中使用线程池"></a>1. 在RequestHandler的http请求处理方法(get/post等)中使用线程池</h1><p>线程池为RequestHandler持有，请求处理逻辑中的耗时/阻塞任务可以提交给线程池处理，主循环逻辑可以继续处理其他请求，线程池内的任务处理完毕后，会通过回调注册callback到ioloop，ioloop可以通过执行callback恢复挂起的请求处理逻辑。</p>
<pre><code>import tornado.web
import tornado.gen
from tornado.concurrent import run_on_executor
from concurrent.futures import ThreadPoolExecutor
class HasBlockTaskHandler(tornado.web.RequestHandler):
    executor = ThreadPoolExecutor(4) #起线程池，由当前RequestHandler持有
    @tornado.gen.coroutine
    def get(self):
        ...
        result = yield self.block_task() #block_task将提交给线程池运行
        ... #继续处理
        self.write(response)
    @run_on_executor
    def block_task(self):
        time.sleep(5) #也可能是其他耗时／阻塞型任务
        return result #直接return结果即可
</code></pre><h1 id="2-项目使用全局线程池，将http请求处理方法-get-post等-整个托管给线程池"><a href="#2-项目使用全局线程池，将http请求处理方法-get-post等-整个托管给线程池" class="headerlink" title="2. 项目使用全局线程池，将http请求处理方法(get/post等)整个托管给线程池"></a>2. 项目使用全局线程池，将http请求处理方法(get/post等)整个托管给线程池</h1><p>假如某个RequestHandler中的http method中有可能导致主处理逻辑阻塞的任务，直接将该http method整个托管给线程池执行。</p>
<pre><code>import functools
import tornado.ioloop
import tornado.web
from concurrent.futures import ThreadPoolExecutor
EXECUTOR = ThreadPoolExecutor(max_workers=4)#全局线程池
def unblock(http_method):
    #必须添加该装饰器，表明当前方法结束后，并不finish该请求
    #Tornado请求执行的流程默认是: initialize()-&gt;prepare()-&gt;http_method(get/post等)-&gt;finish()
    #当用unblock装饰器装饰后，http_method实际是执行下面的_wrapper()方法，在_wrapper中我们只是将原始的
    #http_method提交给线程池处理，所以还没有执行完该http_method，所以还不能finish该请求
    @tornado.web.asynchronous 
    @functools.wraps(http_method)
    def _wrapper(self, *args, **kwargs):
        #以下的callback必须在主线程执行
        #self.write(),self.finish()等都不是线程安全的
        def callback(future):
            self.write(future.result())
            self.finish()
        _future = EXECUTOR.submit(
            functools.partial(http_method, self, *args, **kwargs)  
        )
        tornado.ioloop.IOLoop.instance().add_future(_future, callback)
    return _wrapper
class BlockHandler(tornado.web.RequestHandler):
    @unblock
    def get(self): #该方法将被提交到线程池中运行
        ...
        #直接return，该结果即future.result(), 后续将被self.write(result)
        #不要在子线程中执行self.write(),因为这并非线程安全的方法
        #通过ioloop.IOLoop.instance().add_callback的方式，将其交给主线程执行
        ＃ioloop提供的add_callback是线程安全的
        return result
</code></pre><h1 id="3-GIL的问题"><a href="#3-GIL的问题" class="headerlink" title="3. GIL的问题"></a>3. GIL的问题</h1><p>GIL使得Python多线程无法充分利用多核，并且同一时刻只有一个线程工作，那用多线程还有什么意义？这里面其实有一个问题需要注意：web服务通常是IO密集型的，当我们使用线程池的时候，其实大多都是在有同步阻塞的IO任务且没有很好的异步库的时候使用，GIL在当前线程被阻塞在IO任务上时，是可以被释放从而给其他线程运行的机会的，所以使用线程池还是可以大大的提升性能。</p>
<p>总之，在单线程Tornado中使用同步阻塞的IO是一个需要认真对待的问题，对于单个进程来说，同步阻塞的IO意味着当前服务进程（对Tornado来说其实就是主线程）对于IO异常情况（比如有某个第三方请求响应超慢）的承受能力很差，一个请求慢，其后所有的请求都会滞后。但异步或线程池就不会出现这种情况。</p>
<h1 id="4-参考"><a href="#4-参考" class="headerlink" title="4. 参考"></a>4. 参考</h1><ol>
<li><a href="http://www.dongwm.com/archives/shi-yong-tornadorang-ni-de-qing-qiu-yi-bu-fei-zu-sai/" target="_blank" rel="external">使用Tornado让你的请求异步非阻塞</a></li>
<li><a href="https://lbolla.info/blog/2013/01/22/blocking-tornado" target="_blank" rel="external">Blocking tasks in Tornado</a></li>
<li><a href="https://github.com/nikoloss/iceworld" target="_blank" rel="external">Tornado的多线程封装</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Tornado本身的设计目标是单线程异步非阻塞，要想很好的发挥它的性能最好使用异步IO，并且Tornado本身也提供了异步的&lt;code&gt;AsyncHttpClient&lt;/code&gt;的实现，配合&lt;code&gt;gen.coroutine&lt;/code&gt;和&lt;code&gt;yield&lt;/c
    
    </summary>
    
      <category term="Tornado" scheme="https://cyrusin.github.io/categories/Tornado/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Tornado" scheme="https://cyrusin.github.io/tags/Tornado/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>利用Redis作周期性访问失败计数功能</title>
    <link href="https://cyrusin.github.io/2016/03/29/python-redis-counter-20160329/"/>
    <id>https://cyrusin.github.io/2016/03/29/python-redis-counter-20160329/</id>
    <published>2016-03-29T08:46:39.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>每次进入应用客户端时，都需要进行后端鉴权服务，接口会调用某牌照方的鉴权接口，根据用户的MAC地址决定用户是否有权限登陆使用服务。由于调用的接口不是很稳定，有时会出现连续一段时间误判，导致终端大量用户无法使用APP，所以决定在接口这边做一个策略：</p>
<blockquote>
<p>统计一段时间内的第三方鉴权接口鉴权失败数量，当超过某一阈值时，接口暂时对用户请求返回成功。</p>
</blockquote>
<p>由于只是周期性的计数，比如十分钟，所以当第三方服务异常，连续大量用户请求失败的时候，只要同时做好监控报警工作，及时上报给第三方，同时，并不影响用户使用服务。第三方发现后可以及时处理，处理正常后，接口又可以继续以牌照方的响应为准，所以基本也不违背广电总局可管可控的原则。</p>
<p>这种周期性计数功能，使用Redis最好不过。</p>
<p>方案是使用Redis的<code>INCR</code>命令作自增，同时注意“周期性”这个限制是通过<code>EXPIRE</code>命令控制key的有效期，但需注意，使用事务保证命令序列的原子性，防止出现<code>EXPIRE</code>命令执行失败，导致缓存的数据永久有效的现象。</p>
<p>实际使用时，在缓存的key上使用一定的技巧，如：统计十分钟内的数据，在生成key的时候，用当前时间（以秒为单位）除以600（十分钟），把结果拼到key里，这样可以保证：即使Redis事务执行都没有成功， 依然可以每十分钟更新一次计数器，使得当第三方接口恢复正常后，用户的请求依然以第三方接口的响应为准。</p>
<p>以下是代码：</p>
<pre><code>current_time = int(time.time())
#COUNTER_INTERVAL_S是计数周期
#当前时间除以计数周期, 保证计数周期的有效性
time_tick = current_time / COUNTER_INTERVAL_S
key = &apos;&apos;.join([&apos;counter:device_login:failed:&apos;, str(time_tick)])
redis_client = redis_cache.client()
failed_num = redis_client.get(key)
if failed_num and int(failed_num) &gt; MAX_FAILED: #超过阈值, 直接返回默认结果
    return self.write(default_content)
else: #没有超过阈值, 计数器自增
    try:
        #注意使用事务
        p = redis_client.pipeline()
        p.incr(key, 1)
        p.expire(key, COUNTER_INTERVAL_S)
        p.execute()
    except Exception, e:
        logging.exception(e)
    return self.write(content)
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;每次进入应用客户端时，都需要进行后端鉴权服务，接口会调用某牌照方的鉴权接口，根据用户的MAC地址决定用户是否有权限登陆使用服务。由于调用的接口不是很稳定，有时会出现连续一段时间误判，导致终端大量用户无法使用APP，所以决定在接口这边做一个策略：&lt;/p&gt;
&lt;blockquot
    
    </summary>
    
      <category term="Redis" scheme="https://cyrusin.github.io/categories/Redis/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
      <category term="Redis" scheme="https://cyrusin.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Python闭包的两个注意事项</title>
    <link href="https://cyrusin.github.io/2016/03/11/python-closure-20160311/"/>
    <id>https://cyrusin.github.io/2016/03/11/python-closure-20160311/</id>
    <published>2016-03-11T08:20:30.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>##延迟绑定</p>
<p>Python闭包函数所引用的外部自由变量是延迟绑定的。</p>
<pre><code>In [2]: def multipliers():
   ...:     return [lambda x: i * x for i in range(4)] 
In [3]: print [m(2) for m in multipliers()]
[6, 6, 6, 6]
</code></pre><p>如以上代码: <code>i</code>是闭包函数引用的外部作用域的自由变量, 只有在内部函数被调用的时候才会搜索变量<code>i</code>的值, 由于循环已结束, <code>i</code>指向最终值3, 所以各函数调用都得到了相同的结果。</p>
<p>解决方法:</p>
<p>1) 生成闭包函数的时候立即绑定(<strong>使用函数形参的默认值</strong>):</p>
<pre><code>In [5]: def multipliers():
    return [lambda x, i=i: i* x for i in range(4)]
       ...: 

In [6]: print [m(2) for m in multipliers()]
[0, 2, 4, 6]
</code></pre><p>如以上代码: 生成闭包函数的时候, 可以看到每个闭包函数都有一个带默认值的参数: <code>i=i</code>, 此时, 解释器会查找<code>i</code>的值, 并将其赋予形参<code>i</code>, 这样在生成闭包函数的外部作用域(即外部循环中), 找到了变量<code>i</code>, 遂将其当前值赋予形参<code>i</code>。</p>
<p>2) 使用<strong>functools.partial</strong>:</p>
<pre><code>In [26]: def multipliers():
    return [functools.partial(lambda i, x: x * i, i) for i in range(4)]
    ....: 

In [27]: print [m(2) for m in multipliers()]
    [0, 2, 4, 6]
</code></pre><p>如以上代码: 在有可能因为延迟绑定而出问题的时候, 可以通过<code>functools.partial</code>构造偏函数, 使得自由变量优先绑定到闭包函数上。</p>
<p>##禁止在闭包函数内对引用的自由变量进行重新绑定</p>
<pre><code>def foo(func):
    free_value = 8
    def _wrapper(*args, **kwargs):
        old_free_value = free_value #保存旧的free_value
        free_value = old_free_value * 2 #模拟产生新的free_value
        func(*args, **kwargs)
        free_value = old_free_value
    return _wrapper
</code></pre><p>以上代码会报错, <code>UnboundLocalError: local variable &#39;free_value&#39; referenced before assignment</code>, 以上代码本意是打算实现一个带有某个初始化状态(<code>free_value</code>)但在执行内部闭包函数的时候又可以按需变化出新的状态(<code>free_value = old_free_value * 2</code>)的装饰器, 但内部由于发生了重新绑定, 解释器会将<code>free_value</code>看作局部变量, <code>old_free_value = free_value</code>则会报错, 因为解释器认为<code>free_value</code>是没有赋值就被引用了。</p>
<p>解决：打算修改闭包函数引用的自由变量时, 可以将其放入一个list, 这样, <code>free_value = [8]</code>, <code>free_value</code>不可修改, 但<code>free_value[0]</code>是可以安全的被修改的。</p>
<p>另外, Python 3.x增加了<code>nonlocal</code>关键字, 也可以解决这个问题。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;##延迟绑定&lt;/p&gt;
&lt;p&gt;Python闭包函数所引用的外部自由变量是延迟绑定的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;In [2]: def multipliers():
   ...:     return [lambda x: i * x for i in range(4)
    
    </summary>
    
      <category term="Python" scheme="https://cyrusin.github.io/categories/Python/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>打破递归栈的深度限制: 解析一种Python尾递归优化的方法</title>
    <link href="https://cyrusin.github.io/2015/12/08/python-20151208/"/>
    <id>https://cyrusin.github.io/2015/12/08/python-20151208/</id>
    <published>2015-12-08T09:26:53.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>Python的递归调用栈的深度有限制, 可以通过<code>sys.getrecursionlimit()</code>查看。</p>
<p>尾递归在很多语言中都可以被编译器优化, 基本都是直接复用旧的执行栈, 不用再创建新的栈帧, 原理上其实也很简单, 因为尾递归在本质上看的话递归调用是整个子过程调用的最后执行语句, 所以之前的栈帧的内容已经不再需要, 完全可以被复用。</p>
<p>需要注意的是, 一定记住尾递归的特点是: 递归调用是整个子过程调用的最后一步, 否则就不是真正的尾递归了, 如下就不是真正的尾递归, 虽然递归调用出现在尾部:</p>
<pre><code>def fib(n):
    if n == 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fib(n-1) + fib(n-2)
</code></pre><p>很明显递归调用并不是整个计算过程的最后一步, 计算<code>fib(n)</code>是需要先递归求得<code>fib(n-1)</code>和<code>fib(n-2)</code>, 然后做一步加法才能得到最终的结果。</p>
<p>如下是尾递归:</p>
<pre><code>def fib(n, a, b):
    if n == 1:
        return a
    else:
        return fib(n-1, b, a+b)
</code></pre><p>关于Python中的尾递归调用有一段<a href="http://code.activestate.com/recipes/474088/" target="_blank" rel="external">神奇的代码</a>:</p>
<pre><code>import sys

class TailCallException:
    def __init__(self, args, kwargs):
        self.args = args
        self.kwargs = kwargs

def tail_call_optimized(func):
    def _wrapper(*args, **kwargs):
        f = sys._getframe()
        if f.f_back and f.f_back.f_back and f.f_code == f.f_back.f_back.f_code:
            raise TailCallException(args, kwargs)

        else:
            while True:
                try:
                    return func(*args, **kwargs)
                except TailCallException, e:
                    args = e.args
                    kwargs = e.kwargs
    return _wrapper

@tail_call_optimized
def fib(n, a, b):
    if n == 1:
        return a
    else:
        return fib(n-1, b, a+b)

r = fib(1200, 0, 1) #突破了调用栈的深度限制
</code></pre><p>以上的代码是怎样的工作的呢？</p>
<p>理解它需要对Python虚拟机的函数调用有一定的理解。其实以上代码和其他语言对尾递归的调用的优化原理都是相似的,那就是在尾递归调用的时候重复使用旧的栈帧, 因为之前说过, 尾递归本身在调用过程中, 旧的栈帧里面那些内容已经没有用了, 所以可以被复用。</p>
<p>Python的函数调用首先要了解<code>code object, function object, frame object</code>这三个object(对象), <code>code object</code>是静态的概念, 是对一个可执行的代码块的抽象, module, function, class等等都会被生成<code>code object</code>, 这个对象的属性包含了”编译器”(Python是解释型的，此处的编译器准确来说只是编译生成字节码的)对代码的静态分析的结果, 包含字节码指令, 常量表, 符号表等等。<code>function object</code>是函数对象, 函数是第一类对象, 说的就是这个对象。当解释器执行到<code>def fib(...)</code>语句的时候(<code>MAKE_FUNCTION</code>), 就会基于<code>code object</code>生成对应的<code>function object</code>。</p>
<p>但是生成<code>function object</code>并没有执行它, 当真正执行函数调用的时候, <code>fib(...)</code>这时候对应的字节码指令(<code>CALL_FUNCITON</code>), 可以看一下, CPython的源码, 真正执行的时候Python虚拟机会模拟x86CPU执行指令的大致结构, 而运行时栈帧的抽象就是<code>frame obejct</code>, 这玩意儿就模拟了类似C里面运行时栈, 寄存器等等运行时状态, 当函数内部又有函数调用的时候, 则又会针对内部的嵌套的函数调用生成对应的<code>frame object</code>, 这样看上去整个虚拟机就是一个栈帧连着又一个栈帧, 类似一个链表, 当前栈帧通过<code>f_back</code>这个指针指向上一栈帧, 这样你才能在执行完毕, 退出当前帧的时候回退到上一帧。和C里执行栈的增长退出模式很像。</p>
<p><code>frame object</code>栈帧对象只有在当前函数执行的时候才会产生, 所以你只能在函数内通过<code>sys._getframe()</code>调用来获取当前执行帧对象。通过<code>f.f_back</code>获取上一帧, <code>f.f_back.f_back</code>来获取当前帧的上一帧的上一帧(当前帧的“爷爷”)。</p>
<p>另外一个需要注意到的是, 对于任何对尾递归而言, 其执行过程可以线性展开, 此时你会发现,  最终结果的产生完全可以从任意中间状态开始计算, 最终都能得到同样的执行结果。如果把函数参数看作状态(<code>state_N</code>)的话, 也就是<code>tail_call(state_N)-&gt;tail_call(state_N-1)-&gt;tail_call(state_N-2)-&gt;...-&gt;tail_call(state_0)</code>, <code>state_0</code>是递归临界条件, 也就是递归收敛的最终状态, 而你在执行过程中, 从任一起始状态(<code>state_N</code>)到收敛状态(<code>state_0</code>)的中间状态<code>state_x</code>开始递归, 都可以得到同样的结果。</p>
<p>当Python执行过程中发生异常(错误)时(或者也可以直接手动抛出<code>raise ...</code>), 该异常会从当前栈帧开始向旧的执行栈帧传递, 直到有一个旧的栈帧捕获这个异常, 而该栈帧之后(比它更新的栈帧)的栈帧就被回收了。</p>
<p>有了以上的理论基础, 就能理解之前代码的逻辑了:</p>
<ol>
<li><p>尾递归函数fib被tail_call_optimized装饰, 则fib这个名字实际所指的function object变成了tail_call_optimized里return的_wrapper, fib 指向_wrapper。</p>
</li>
<li><p>注意_wrapper里return func(*args, **kwargs)这句, 这个func还是未被tail_call_optimized装饰的fib（装饰器的基本原理）, func是实际的fib, 我们称之为real_fib。</p>
</li>
<li><p>当执行fib(1200, 0, 1)时, 实际是执行_wrapper的逻辑, 获取帧对象也是_wrapper对应的, 我们称之为frame_wapper。</p>
</li>
<li><p>由于我们是第一次调用, 所以”if f.f_back and f.f_back.f_back and f.f_code == f.f_back.f_back.f_code”这句里f.f_code==f.f_back.f_back.f_code显然不满足。</p>
</li>
<li><p>继续走循环, 内部调用func(*args, **kwargs), 之前说过这个func是没被装饰器装饰的fib, 也就是real_fib。</p>
</li>
<li><p>由于是函数调用, 所以虚拟机会创建real_fib的栈帧, 我们称之为frame_real_fib, 然后执行real_fib里的代码, 此时当前线程内的栈帧链表按从旧到新依次为:</p>
<pre><code>旧的虚拟机栈帧，frame_wrapper，frame_real_fib(当前执行帧)
</code></pre></li>
</ol>
<p>real_fib里的逻辑会走return fib(n-1, b, a+b), 有一个嵌套调用, 此时的fib是谁呢？此时的fib就是我们的_wrapper, 因为我们第一步说过, fib这个名字已经指向了_wrapper这个函数对象。</p>
<ol>
<li><p>依然是函数调用的一套, 创建执行栈帧, 我们称之为frame_wrapper2, 注意： 执行栈帧是动态生成的, 虽然对应的是同样函数对象(_wrapper), 但依然是不同的栈帧对象, 所以称之为frame_wrapper2。 今后进入frame_wrapper2执行, 注意此时的虚拟机的运行时栈帧的结构按从旧到新为: </p>
<pre><code>旧的虚拟机栈帧、frame_wrapper、frame_real_fib、frame_wrapper2(当前执行栈帧)
</code></pre></li>
<li><p>进入frame_wrapper2执行后, 首先获取当前执行帧, 即frame_wrapper2, 紧接着, 执行判断, 此时:</p>
<pre><code>if f.f_back and f.f_back.f_back and f.f_code == f.f_back.f_back.f_code
</code></pre></li>
</ol>
<p>以上这句就满足了, f.f_code是当前帧frame_wrapper2的执行帧的code对象, f.f_back.f_back.f_code从当前的执行帧链表来看是frame_wrapper的执行帧的code对象, 很显然他们都是同一个code块的code object(def _wrapper…..)。于是抛出异常, 通过异常的方式, 把传过来的参数保留, 然后, 异常向旧的栈帧传递, 直到被捕获, 而之后的栈帧被回收, 即抛出异常后, 直到被捕获时, 虚拟机内的执行帧是:</p>
<pre><code>旧的虚拟机栈帧、frame_wrapper(当前执行帧)
</code></pre><p>于是现在恢复执行frame_wrapper这个帧, 直接顺序执行了, 由于是个循环, 同时参数通过异常的方式被捕获, 所以又进入了return func(*args, **kwargs)这句, 根据我们之前说的, 尾递归从递归过程中任意中间状态都可以收敛到最终状态, 所以就这样, 执行两个帧, 搞出中间状态, 然后抛异常, 回收两个帧, 这样一直循环直到求出最终结果。</p>
<p>在整个递归过程中, 没有频繁的递归一次, 生成一个帧, 如果你不用这个优化, 可能你递归1000次, 就要生成1000个栈帧, 一旦达到递归栈的深度限制, 就挂了。</p>
<p>使用了这个装饰器之后, 最多生成3个帧, 随后就被回收了, 所以是不可能达到递归栈的深度的限制的。</p>
<p>注意： <strong>这个装饰器只能针对尾递归使用。</strong></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Python的递归调用栈的深度有限制, 可以通过&lt;code&gt;sys.getrecursionlimit()&lt;/code&gt;查看。&lt;/p&gt;
&lt;p&gt;尾递归在很多语言中都可以被编译器优化, 基本都是直接复用旧的执行栈, 不用再创建新的栈帧, 原理上其实也很简单, 因为尾递归在本质上
    
    </summary>
    
      <category term="Python" scheme="https://cyrusin.github.io/categories/Python/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Nginx与Lua: 一种利用error_page实现鉴权服务故障转移的尝试</title>
    <link href="https://cyrusin.github.io/2015/11/27/lua-20151127/"/>
    <id>https://cyrusin.github.io/2015/11/27/lua-20151127/</id>
    <published>2015-11-27T06:25:05.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>为了响应总菊的要求, 互联网智能电视盒子必须接入牌照方的播控平台, 登陆时必须认证一下, 认证通过才让你接入我们的服务, 否则你就呵呵了。我们有两个接口会代理一下用户的请求, 大致逻辑是:</p>
<blockquote>
<p>客户端首先请求接口A, 会把MAC地址带过来, 接口A的handler会做一些校验, 然后根据一些其他的参数, 重新组装一个请求去访问牌照方的接口, 根据参数的不同会请求不同的接口<br>牌照方会根据用户的MAC地址和其他信息返回鉴权的结果, 返回里面会有某个字段表示成功还是失败<br>接口A对牌照方的接口做些校验, 没问题的返给客户端, 客户端根据鉴权结果, 成功则继续进行剩下的鉴权，失败则终止请求, 用户可能就无法使用剩下的服务</p>
</blockquote>
<p>看起来是个很简单的只读性的接口, 一开始的时候把这个东西和其他的业务放在一起, 但是这个接口相当敏感, 尤其是在每天的使用高峰的时候, 这个时候盒子的开启操作数量很大, 由于每次开启进入盒子都要做认证, 所以这个接口的请求量随着盒子用户的增加也是与日剧增, 关键是高峰期的时候难免接口响应的会慢一点, 这时候根据一开始的设计, 用户总是无法进入盒子, 什么操作也做不了, 只能投诉客服。</p>
<p>这个接口, 按要求是不允许使用缓存的, 因为上面要求鉴权必须实时生效。后来我们追查这个接口超时多的原因, 发现调用的牌照方的接口性能不是很好, 请求量稍微多点, 比如每天的高峰期、周末, 基本就瘫掉了。超时一大堆, 接口也跟着超时一大堆, 一到高峰期就504, 投诉量太多, 甚至惊动了我们部门的老大。</p>
<p>先是把这个服务独立出来, 但是由于超时来自上游, 我们在调接口的时候虽然加了超时就自动返回默认鉴权成功的功能, 但由于高峰期超时数量实在太大, 超时后又得打日志什么的, 反而服务自己的IO好像又影响了Tornado的性能, 不管你怎么优化, Tornado就好象有一个瓶颈, 始终跨不过那道槛。高峰期504还是有一些。</p>
<p>都知道Tornado是异步非阻塞的高性能服务器, 但在这个问题上, 我一直觉得我们使用的方式有问题, 就喜欢加进程数, 以为进程多了就一定好, 根据我自己的实验, 从压测的结果来看, 当进程数远大于你的CPU核数的时候, 其实性能是不升反而又微弱下降的, 而且进程数太多, 反而超时的请求多了起来。我的理解是, 当进程数太多的时候, CPU花费了大量时间在进程切换上, 使得每个进程占有CPU的平均时间反而少了, 造成了在一定时间内, 无谓等待的请求反而多了, 超时也就来了。</p>
<p>这次我们老大也被这个问题搞得挺纠结, 我决定好好分析解决一下这个问题。又轮到哥出手了。</p>
<p>我只好拿出杀招了, 之前他们总是把焦点聚集到Tornado和Tornado的上游, 我的思路是, 人家的东西超时, 你期待人家替你改, 这就成了坐以待毙了。现在的问题就是: Tornado总是在高峰期超时, 客户端面对504没有好好处理, 保守的认为是鉴权失败, 用户就什么也干不了了。</p>
<p>这次我解决这个问题继续使用我的暗器, Lua, 我们有一部分服务的接入过滤使用了Lua在Nginx中处理逻辑, 不论可维护性还是性能都很好。这次我还是用Lua。具体解决方案就是:</p>
<blockquote>
<p>Nginx捕获upstream的异常(504, 502)等, 内部跳转, 由Lua接管异常请求, 通过Lua实现一部分接口的逻辑, 根据不同的请求返回默认鉴权成功的结果, 优先让用户使用服务。</p>
</blockquote>
<p>由于超时主要发生在高峰期, 所以平时的时候只有极其微小的概率Tornado会超时, 所以这个基本是不影响正常鉴权逻辑的。只是在高峰期的时候, 当Tornado及Tornado的上游扛不住的时候, 我们在Nginx里内嵌的Lua会把异常请求接管, 然后返回给用户默认成功的结果, 这样用户就不会在高峰期由于服务响应的问题, 被踢掉了。</p>
<p>实践证明Nginx结合Lua的这种解决方案在线上取得了很好的效果, 毕竟Nginx的性能是足足的, 而Lua也是名副其实的快, 这俩黄金组合的搭配可以说基本实现了全天服务无504, 高峰期也是兢兢业业, 十分扛打, 从日志来看, 牌照方依然在每天给你千八个超时, Tornado在高峰期依然会有搞不定的赶脚, 但这些通通都被Nginx和Lua洗地了, Nginx和Lua实在是十分可靠的接盘侠。</p>
<p>部分代码:</p>
<pre><code>location ~ /url {
    access_by_lua_file &lt;path&gt;/check_***.lua; #Lua做校验

    proxy_pass http://tornado;
    proxy_redirect      default;
    proxy_set_header    X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header    X-Real-IP $x_remote_addr;
    proxy_set_header    Host $http_host;
    proxy_set_header    Range $http_range;

    error_page 502 504 =200 @device_entry_default; #故障转移
} 

location @device_entry_default {
    #lua来接管异常请求
    content_by_lua &apos;  
        local device_entry = &quot;&quot;
        if ngx.var.arg_pid==... then
            ...
        elseif ngx.var.arg_static==&quot;0&quot; then
            ...
        else
            ...
        end
        ngx.header.content_type = &quot;application/json; charset=UTF-8&quot;
        ngx.say(default_entry)
    &apos;;
}
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;为了响应总菊的要求, 互联网智能电视盒子必须接入牌照方的播控平台, 登陆时必须认证一下, 认证通过才让你接入我们的服务, 否则你就呵呵了。我们有两个接口会代理一下用户的请求, 大致逻辑是:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;客户端首先请求接口A, 会把MAC地址带过来
    
    </summary>
    
      <category term="Nginx" scheme="https://cyrusin.github.io/categories/Nginx/"/>
    
    
      <category term="项目" scheme="https://cyrusin.github.io/tags/%E9%A1%B9%E7%9B%AE/"/>
    
      <category term="Nginx" scheme="https://cyrusin.github.io/tags/Nginx/"/>
    
      <category term="Lua" scheme="https://cyrusin.github.io/tags/Lua/"/>
    
  </entry>
  
  <entry>
    <title>Linux的IO多路复用: select/poll/epoll</title>
    <link href="https://cyrusin.github.io/2015/11/15/iomultiplexing-20151115/"/>
    <id>https://cyrusin.github.io/2015/11/15/iomultiplexing-20151115/</id>
    <published>2015-11-15T08:31:10.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>Linux里的IO多路复用是有效提高IO效率的技术。主要有select、poll、epoll三种。</p>
<h1 id="select"><a href="#select" class="headerlink" title="select"></a>select</h1><p>select调用的函数接口是:</p>
<pre><code>int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
</code></pre><p>参数说明:</p>
<blockquote>
<p>nfds: fdset中最大描述符值加1, fdset是一个位数组, 大小为__FD_SETSIZE(1024), 位数组的每一位表示该描述符是否被检查<br>readfds, writefds, exceptfds: 三个位数组, 非别对应监听不同类型读写及错误事件的描述符。可被内核修改用于通知相应事件<br>timeout: 超时时间, 可被内核修改, 则其值是超时剩余时间</p>
</blockquote>
<p>基本原理:</p>
<blockquote>
<p>用户进程调用select后阻塞, select对应内核态的sys_select, 将readfds, writefds, exceptfds对应的fd_set拷贝到内核, 内核通过轮询的方式检查注册的描述符, 对于事件发生的描述符记录到临时fd_set, 最终会拷贝到用户进程空间，轮询完毕后返回。<br>没有事件发生时, 睡眠到超时返回。然后进行下次轮询<br>select对于监视的描述符有数量限制, __FD_SETSIZE(1024)个</p>
</blockquote>
<h1 id="poll"><a href="#poll" class="headerlink" title="poll"></a>poll</h1><p>poll的函数接口:</p>
<pre><code>int poll(struct pollfd *fds, nfds_t fds, int timeout);

struct pollfd {
    int fd; /*描述符*/
    short events; /*关注的事件*/
    short revents; /*内核报告的实际发生的事件*/
};
</code></pre><p>参数说明:</p>
<blockquote>
<p>fds: 结构体数组, 每一个结构体表示一个描述符及该描述符上的关注事件、发生的事件</p>
</blockquote>
<p>基本原理:</p>
<blockquote>
<p>用户进程向内核传递fds数组, 不像select一样有描述符数目限制<br>poll对应的是内核的sys_poll, 依然是对fds数组中的每个pollfd进行poll, 检查其事件发生, 修改revents字段报告该事件<br>poll返回后, 逐一检查fds数组中每个pollfd的revents</p>
</blockquote>
<h1 id="epoll"><a href="#epoll" class="headerlink" title="epoll"></a>epoll</h1><p>epoll的函数接口:</p>
<pre><code>int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);

typedef union epoll_data {
    void *ptr;
    int fd;
    __uint32_t u32;
    __uint64_t u64;
}epoll_data_t;

typedef struct epoll_event {
    __uint32_t events;
    epoll_data data;
};
</code></pre><p>参数说明:</p>
<blockquote>
<p>size: epoll并不限制监视的文件描述符数目, 这只是对内核的初始化建议<br>events: epoll_wait的第二个参数用于存放结果</p>
</blockquote>
<p>基本原理:</p>
<blockquote>
<p>用户进程通过epoll_create创建epoll描述符, 返回的int型就是epoll描述符的值, -1表示创建失败<br>通过epoll_ctl为某个fd进行添加、删除、修改事件, 即op为EPOLL_CTL_ADD, EPOLL_CTL_DEL, EPOLL_CTL_MOD, 关联的事件为event<br>epoll_wait等待io事件, 该调用会返回活跃描述符(发生了监听事件的描述符)</p>
</blockquote>
<h1 id="epoll相对于select和poll的优点"><a href="#epoll相对于select和poll的优点" class="headerlink" title="epoll相对于select和poll的优点"></a>epoll相对于select和poll的优点</h1><ol>
<li>相对于select, epoll监视描述符没有数量限制, 理论上是你的进程可以打开的描述符的最大数目;</li>
<li>IO效率并不随监视描述符数目增加而明显下降, <strong>epoll并不像select和poll那样轮询</strong>, 而是<strong>通过给每个监视的文件描述符注册回调函数</strong>实现, 当某个监视的文件描述符上发生了监视的事件后, 触发回调将自己写入结果中, 这样每次epoll返回的总是活跃的文件描述符集, 避免了在select和poll的使用中用户依然需要逐一检查描述符导致的性能下降;</li>
<li>对监视事件有电平触发和边沿触发两种触发方式, 其中边沿触发的性能极高, 只是应用逻辑会稍复杂;</li>
<li>内核和用户进程通过共享内存的方式传递信息, 主要是内核和用户空间通过Linux的mmap映射同一块内存, 这样减少了多次内存拷贝的性能开销。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux里的IO多路复用是有效提高IO效率的技术。主要有select、poll、epoll三种。&lt;/p&gt;
&lt;h1 id=&quot;select&quot;&gt;&lt;a href=&quot;#select&quot; class=&quot;headerlink&quot; title=&quot;select&quot;&gt;&lt;/a&gt;select&lt;/h1&gt;
    
    </summary>
    
      <category term="服务器" scheme="https://cyrusin.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
    
      <category term="网络IO" scheme="https://cyrusin.github.io/tags/%E7%BD%91%E7%BB%9CIO/"/>
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="网络编程" scheme="https://cyrusin.github.io/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Linux" scheme="https://cyrusin.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>两种协程的调度方式: trampoline、thread scheduler</title>
    <link href="https://cyrusin.github.io/2015/11/06/coroutine-20151106/"/>
    <id>https://cyrusin.github.io/2015/11/06/coroutine-20151106/</id>
    <published>2015-11-06T07:48:26.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>协程是用户态内的,或者准确点说是线程内部的一种上下文切换技术,由于协程切换是在用户态下完成的,所以省去了线程切换时频繁出入内核态的资源开销,可以形成一种很高效的协作式并发技术。</p>
<p>这个简短的视频介绍了一些有关协程、并发之类的东西,很有意义。</p>
<p><a href="https://www.youtube.com/watch?v=b7R3-_ViNxk" target="_blank" rel="external">Coroutines, event loops, and the history of Python generators</a></p>
<p>从里面学习到两种很好的协程的调度方式。把代码拿过来分享一下。</p>
<p>##Coroutine trampoline<br>这种方式下的协程调度比较好理解,就是从一个初始状态开始,一条执行线索不断的在多个协程之间切换,就好像多个协程协作完成一项任务。<br>代码:</p>
<pre><code>def co_trampoline(generators, start, init_data):
    coroutines = dict() #上下文映射,其实就是从名字到协程对象的映射
    for g in generators:
        if g is start: #初始协程
            coroutine = g(init_data)
            #curr, data =&gt; 当前需要切换到的协程, 传给协程的数据
            curr, data = coroutine.next()
        else:
            coroutine = g()
            coroutine.next()
            coroutines[coroutine.__name__] = coroutine

    while True: #一定是循环的执行
        try:
            #每个协程抛出下一个将要执行的协程和数据,相当于上下文切换
            curr, data = coroutines[curr].send(data)
        except StopIteration:
            break
</code></pre><p>##Thread scheduler<br>这种方式类似一种用协程去模拟多个线程的并发模式,所以叫”Thread scheduler”。<br>代码:</p>
<pre><code>from collections import defaultdict
def thread_scheduler(generators):
    #协程要消费的数据,比如实际应用时可以是一个任务队列、消息队列之类的
    thread_data = defaultdict(list)
    threads = [g() for g in generators]
    while True:
        try:
            for t in threads:
                data = thread_data[t.__name__]
                #协程t(模拟一个&quot;线程&quot;)执行,并返回t下一步的协程和其数据
                consumer, data = t.send(data)
                thread_data[consumer].append(data) #任务队列增加,等待调度
        except StopIteration:
            break
</code></pre><p>假如我的<code>generators</code>里有三个协程<code>c1</code>, <code>c2</code>, <code>c3</code>, 那么以上代码的执行顺序就是<code>c1-&gt;c2-&gt;c3-&gt;c1-&gt;c2-&gt;c3...</code>, 不停的循环执行,每一个协程内部可以根据处理的过程不同,返回不同的下一步的协程。就像一个带状态转移的有限状态机一样。</p>
<p>##协程的实现<br>之前说了协程像一个状态转移的有限状态机,其实现大致类似下面的样子,上面说的两种协程调度方式都可以依赖这种协程的实现方式:</p>
<pre><code>def coroutine():
    while True:
        state, somedata = process(data) 
        if state == 1:
            data = yield (&apos;c1&apos;, somedata)
        elif state == 2:
            data = yield (&apos;c2&apos;, somedata)
        else:
            break
        ...
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;协程是用户态内的,或者准确点说是线程内部的一种上下文切换技术,由于协程切换是在用户态下完成的,所以省去了线程切换时频繁出入内核态的资源开销,可以形成一种很高效的协作式并发技术。&lt;/p&gt;
&lt;p&gt;这个简短的视频介绍了一些有关协程、并发之类的东西,很有意义。&lt;/p&gt;
&lt;p&gt;&lt;a 
    
    </summary>
    
      <category term="Python" scheme="https://cyrusin.github.io/categories/Python/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>iter(a) is iter(a)?</title>
    <link href="https://cyrusin.github.io/2015/10/18/iter20151018/"/>
    <id>https://cyrusin.github.io/2015/10/18/iter20151018/</id>
    <published>2015-10-18T13:42:32.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>##”可迭代对象”与”迭代器”</p>
<p>Python中<strong>迭代器</strong>指的是实现了<code>__iter__</code>协议与<code>next()</code>方法的对象,其中<code>__iter__</code>协议中返回对象自身(<code>self</code>),<code>next()</code>方法每次返回单一元素,直至所有元素遍历结束,抛出’StopIteration’异常结束。</p>
<p><strong>可迭代对象</strong>是一种更宽泛的定义,只要对象实现了<code>__iter__</code>协议,返回一个<strong>迭代器</strong>(并不像迭代器一样必须返回自身<code>self</code>)用来遍历自己的元素集合,那这个对象就是<strong>可迭代的对象</strong>,也意味着可以直接通过<code>for item in obj...</code>的方式遍历。</p>
<p>##iter(a) is iter(a)?</p>
<pre><code>&gt;&gt;a = [1, 2, 3]
&gt;&gt;iter(a) is iter(a) #(1)
&gt;&gt;False
&gt;&gt;b = iter(a)
&gt;&gt;iter(b) is iter(b) #(2)
&gt;&gt;True
</code></pre><p>以上代码中的<code>a</code>可以是任意container型对象(list, tuple, set, dict…)。可以看到(1) 和 (2)的结果是不同的。</p>
<p>原因是:</p>
<blockquote>
<p><code>iter</code>调用意味着获取对象的迭代器(即<code>__iter__</code>协议返回的迭代器)<br>container实现了<code>__iter__</code>协议,是可迭代的对象,通过<code>iter(a)</code>调用,返回了对象<code>a</code>的迭代器<br>每次<code>iter(a)</code>都返回一个<code>a</code>的迭代器,是不同的对象(虽然他们功能一样),所以(1)是<code>False</code>,因为他们是不同的对象<br>(2)对一个迭代器进行<code>iter</code>调用,根据<strong>迭代器</strong>的定义,迭代器的<code>__iter__</code>协议返回的是迭代器对象自身<br>所以(2)iter(b) is iter(b)是True, 因为b是迭代器,每次iter(b)返回的都是b自身</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;##”可迭代对象”与”迭代器”&lt;/p&gt;
&lt;p&gt;Python中&lt;strong&gt;迭代器&lt;/strong&gt;指的是实现了&lt;code&gt;__iter__&lt;/code&gt;协议与&lt;code&gt;next()&lt;/code&gt;方法的对象,其中&lt;code&gt;__iter__&lt;/code&gt;协议中返回对象自身
    
    </summary>
    
      <category term="Python" scheme="https://cyrusin.github.io/categories/Python/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Nginx与Lua: 使用Nginx的upload模块打造文件上传服务</title>
    <link href="https://cyrusin.github.io/2015/10/13/lua-20151013/"/>
    <id>https://cyrusin.github.io/2015/10/13/lua-20151013/</id>
    <published>2015-10-13T06:47:19.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>客户端提出搞一个日志上传的接口,方便向服务端提交日志,研究了一下,决定用Nginx嵌入Lua的方式搞。</p>
<p>Lua是一个可以嵌入到Nginx配置文件中的动态脚本语言,从而可以在Nginx请求处理的任何阶段执行各种Lua代码。业余时间使用过,感觉小巧轻快,代码风格和Python一样简洁优雅。在Nginx中内嵌Lua,需要你重新编译安装Nginx,加上ngx_lua模块,同时需要安装LuaJIT,可以看看这篇文章”<a href="http://huoding.com/2012/08/31/156" target="_blank" rel="external">Nginx与Lua</a>“。</p>
<p>安装好相关的模块后,Nginx配置如下:</p>
<pre><code>location ~ /upload { # 调用的路由

    # 转到后台处理URL 
    upload_pass /uploadHandle;
    # 临时保存路径
    upload_store /opt/upload/tmp;

    #上传文件的权限,rw表示读写,r只读
    upload_store_access user:rw group:rw all:rw;

    # 是否允许nginx上传参数到后台
    upload_pass_args on;

    # 这里写入http报头，pass到后台页面后能获取这里set的报头字段
    upload_set_form_field &quot;file_name&quot; $upload_file_name;
    upload_set_form_field &quot;file_content_type&quot; $upload_content_type;
    upload_set_form_field &quot;file_tmp_path&quot; $upload_tmp_path;

    # Upload模块自动生成的一些信息，如文件大小与文件md5值
    upload_aggregate_form_field &quot;file_md5&quot; $upload_file_md5;
    upload_aggregate_form_field &quot;file_size&quot; $upload_file_size;

    # 允许的字段，允许全部可以 &quot;^.*$&quot;
    upload_pass_form_field &quot;^submit$|^description$&quot;;
    # 每秒字节速度控制，0表示不受控制，默认0 
    upload_limit_rate 0;

    # 如果pass页面是以下状态码，就删除此次上传的临时文件
    upload_cleanup 400 404 499 500-505;
}

# 后台处理路由,使用Lua
location ~  /uploadHandle {
    lua_need_request_body on; 
    content_by_lua_file /opt/nginx/luas/onupload.lua; # Lua脚本的位置
    return 302 &apos;http://***********&apos;; # 处理完重定向到展示页面
}   
</code></pre><p>客户端直接post提交过来就行,下面是用于处理提交过来的文件的Lua脚本:</p>
<pre><code>function onupload()
    ngx.req.read_body();
    local post_args=ngx.req.get_post_args();
    local table_params=getFormParams(post_args);

    ret_val=processFile(table_params);
    if (ret_val==0) then
        ngx.say(&quot;Boy, upload success!&quot;)
    else
        ngx.say(&quot;Something wrong with nginx!!&quot;)
    end
end

function processFile(params)
    local root_dir=&quot;/opt/upload/&quot;; -- 文件目录
    local filename=params[&quot;file_name&quot;];
    local mv_command=&quot;mv &quot;..trim(params[&quot;file_tmp_path&quot;])..&quot; &quot;..root_dir..filename;

    if (os.execute(mv_command)~=0)then
        ngx.exec(&quot;/50x.html&quot;);
        return 1;
    else
        return 0;
    end
end

function trim(str)
    if (str~=nil)then
        return string.gsub(str, &quot;%s+&quot;, &quot;&quot;);
    else
        return nil;
    end
end

-- [[提交过来的表单数据是一个超长的字符串,
    需要从这个字符串中解析出字典结构的数据,
    这样可以利用key来访问字典中对应的值
]]
function getFormParams(post_args)
    local table_params={};
    for key, val in pairs(post_args) do
        str_params = key ..val
    end

    local str_start = &quot; name&quot;;  
    local str_start_len = string.len(str_start);  
    local str_end = &quot;%-%-&quot;;  
    local str_sign = &quot;\&quot;&quot;;  
    local idx,idx_end = string.find(str_params,str_start);  
    local i = 0;  

    -- 如果字符串内仍有开始标记，则说明还有内容需要分离。继续分离到没内容为止。  
    while idx do  
        str_params = string.sub(str_params,idx_end); -- 截取开始标记后所有字符待用  
        i = string.find(str_params,str_sign); -- 查找字段名开始处的引号索引  
        str_params = string.sub(str_params,i+1); -- 去掉开始处的引号  
        i = string.find(str_params,str_sign); -- 查找字段名结束位置的索引  
        f_name = string.sub(str_params,0,i-1); -- 截取到字段名称                                          
        str_params = string.sub(str_params,i+1); -- 去掉名称字段以及结束时的引号  
        i,i2 = string.find(str_params,str_end); -- 查找字段值结尾标识的索引  
        f_value = string.sub(str_params,1,i-1); -- 截取到字段值  
        real_value=trim(f_value)
        table_params[f_name] = real_value;  
        idx = string.find(str_params,str_start,0); -- 查找判断下一个字段是否存在的  
    end
    local root_dir=&quot;/opt/upload/&quot;;
    table_params[&quot;file_path&quot;]=root_dir..table_params[&quot;file_name&quot;]
    return table_params
end

onupload();
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;客户端提出搞一个日志上传的接口,方便向服务端提交日志,研究了一下,决定用Nginx嵌入Lua的方式搞。&lt;/p&gt;
&lt;p&gt;Lua是一个可以嵌入到Nginx配置文件中的动态脚本语言,从而可以在Nginx请求处理的任何阶段执行各种Lua代码。业余时间使用过,感觉小巧轻快,代码风格和
    
    </summary>
    
      <category term="Nginx" scheme="https://cyrusin.github.io/categories/Nginx/"/>
    
    
      <category term="项目" scheme="https://cyrusin.github.io/tags/%E9%A1%B9%E7%9B%AE/"/>
    
      <category term="Nginx" scheme="https://cyrusin.github.io/tags/Nginx/"/>
    
      <category term="Lua" scheme="https://cyrusin.github.io/tags/Lua/"/>
    
  </entry>
  
  <entry>
    <title>PyCon2015</title>
    <link href="https://cyrusin.github.io/2015/09/23/pycon2015/"/>
    <id>https://cyrusin.github.io/2015/09/23/pycon2015/</id>
    <published>2015-09-23T06:28:55.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>9月19号参加了北京的PyCon2015,有众多互联网公司参加,而且云计算类的公司颇多,足以看出Python在云计算领域的应用之广。</p>
<p>虽然会上广告很多,但也不乏干货:</p>
<blockquote>
<p>1.美团云的分享: 简单语言构建复杂系统 <a href="http://zoomq.qiniudn.com/pychina/PyCon2015China/slides/bj/meituan_simple2complex.pdf" target="_blank" rel="external">幻灯</a></p>
<p>2.Splunk首席工程师丁来强的分享: Python高效函数式编程 <a href="https://github.com/wjo1212/ChinaPyCon2015" target="_blank" rel="external">幻灯</a></p>
<p>3.SpeedyCloud工程师汪尊: Python的多线程与多进程</p>
<p>4.豆瓣的分享: Pidl(Python Interface Description Language)</p>
</blockquote>
<p>同时,可以看到<a href="http://www.tornadoweb.org/en/stable/" target="_blank" rel="external">Tornado</a>这个异步非阻塞高性能轻量级服务器在互联网内有着大范围的应用,Python的胶水特性在云计算领域是粘和多模块的有力武器。</p>
<p><strong>PyCon2015</strong>: <a href="http://cn.pycon.org/2015/beijing.html" target="_blank" rel="external">http://cn.pycon.org/2015/beijing.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;9月19号参加了北京的PyCon2015,有众多互联网公司参加,而且云计算类的公司颇多,足以看出Python在云计算领域的应用之广。&lt;/p&gt;
&lt;p&gt;虽然会上广告很多,但也不乏干货:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1.美团云的分享: 简单语言构建复杂系统 &lt;a hr
    
    </summary>
    
      <category term="Python" scheme="https://cyrusin.github.io/categories/Python/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python中的元类,类,对象</title>
    <link href="https://cyrusin.github.io/2015/09/21/python-oop20150921/"/>
    <id>https://cyrusin.github.io/2015/09/21/python-oop20150921/</id>
    <published>2015-09-21T07:25:28.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>Python的”类型-对象”体系实现的简洁而优雅,”元类”-“类”-“实例对象”自上而下,层次分明。<br>以下是关键的几个点:</p>
<blockquote>
<p>1.一切(函数,类等)皆对象(<code>object</code>)</p>
<p>2.变量名只是对象的引用</p>
<p>3.<code>object</code>类是所有类(对象)的最终父类,但object不是任何类的子类,这是对1的解释</p>
<p>4.<code>type</code>既是<code>object</code>类的实例,也是<code>object</code>类的子类</p>
<p>5.<code>object</code>作为一个类对象,她的类也是<code>type</code></p>
<p>6.<code>object</code>和<code>type</code>是鸡和蛋的关系</p>
<p>7.<code>type</code>很重要,在整个体系中,真正干活的就是<code>type</code></p>
<p>8.元类就是类(一切皆对象)的类</p>
<p>9.1)<code>type</code>负责生成’类’(对象),是最终<strong>元类</strong>(调用:<code>type(class_name, class_parents, attr_dict)</code>),<code>type</code>类有<code>__call__</code>方法,所以她生成的对象(类)都是可调用的</p>
<p>9.2)<code>type</code>负责生成实例对象,是<strong>对象工厂</strong>(让人意外,但事实如此:某类C生成<code>c_instance=C()</code>的过程实质是<code>type.__call__(C,...)</code>,9.1已经解释了,<code>type</code>的对象(C)是可调用的)</p>
<p>10.虽然一切皆对象,但对象生来不同</p>
<p>11.类型对象(包括元类,类)可以被子类化,可以继续被实例化</p>
<p>12.普通实例对象(自定义类型的实例，内建类型的实例)不可以被子类化,不可以被实例化</p>
</blockquote>
<p>Python中对象到底是怎么生成的呢?看代码:</p>
<pre><code>class MetaClass(type):
    def __new__(cls, cls_name, cls_parents, attr_dict):
        print &quot;MetaClass.__new__: create class istance: &quot;, cls_name
        return super(MetaClass, cls).__new__(cls, cls_name, cls_parents, attr_dict)

    def __init__(self, *args, **kwargs):
        print &quot;MetaClass.__init__: initialize: &quot;, self
        super(MetaClass, self).__init__(*args, **kwargs)

    def __call__(self, *args, **kwargs):
        print &quot;MetaClass.__call__: create instance: &quot;, self
        return super(MetaClass, self).__call__(*args, **kwargs)

def cls_decorator(cls):
    print &quot;cls_decorator: &quot;, cls
    return cls

@cls_decorator
class MyClass(object):
    __metaclass__ = MetaClass
    def __new__(cls, *args, **kwargs):
        print &quot;MyClass.__new__, create instance of &quot;, cls
        return super(MyClass, cls).__new__(cls, *args, **kwargs)

    def __init__(self, x):
        print &quot;MyClass.__init__, instance is &quot;, self
        self.x = x

foo = MyClass(1)
</code></pre><p>以上代码执行后的输出:</p>
<pre><code>MetaClass.__new__: create class istance:  MyClass
MetaClass.__init__: initialize:  &lt;class &apos;__main__.MyClass&apos;&gt;
cls_decorator:  &lt;class &apos;__main__.MyClass&apos;&gt;
MetaClass.__call__: create instance:  &lt;class &apos;__main__.MyClass&apos;&gt;
MyClass.__new__, create instance of  &lt;class &apos;__main__.MyClass&apos;&gt;
MyClass.__init__, instance is  &lt;__main__.MyClass object at 0x7fbbd0303ed0&gt;
</code></pre><p>解释如下:</p>
<blockquote>
<ol>
<li><p>Python首先看类声明,准备三个传递给元类的参数。这三个参数分别为类名(cls_name),父类元组(cls_parents)以及属性字典(attr_dict)</p>
</li>
<li><p>Python会检查<code>__metaclass__</code>属性,如果设置了此属性,它将调用metaclass,传递三个参数,并且返回一个类;如果没设此属性,它将去当前类的父类中寻找,如果父类也没有,就去当前类的module里找,如果也没有,直接使用type来充当元类生成类</p>
</li>
<li><p>在这个例子中,MetaClass自身就是一个类,这就意味着生成MyClass这个类的过程和一般的生成对象的过程一致:<code>MetaClass.__new__</code>将首先被调用，输入四个参数，这将新建一个MetaClass类的实例。然后这个实例的<code>MetaClass.__init__</code>将被调用,调用结果是作为一个新的类对象返回。所以此时MyClass将被设置成这个类对象</p>
</li>
<li><p>接下来Python将查看所有装饰了此类的装饰器。在这个例子中,只有一个装饰器。Python将调用这个装饰器,将从元类哪里得到的类传递给它作为参数。然后这个类将被装饰器返回的对象所替代</p>
</li>
<li><p>装饰器返回的类类型与元类设置的相同</p>
</li>
<li><p>当类被调用创建一个新的对象实例时,因为类的类型是MetaClass，因此Python将会调用元类的<code>__call__</code>方法。在这个例子中,<code>MetaClass.__call__</code>只是简单的调用了<code>type.__call__</code>,目的是创建一个传递给它的类的对象实例</p>
</li>
<li><p>下一步<code>type.__call__</code>通过<code>MyClass.__new__</code>创建一个对象</p>
</li>
<li><p>最后<code>type.__call__</code>通过<code>MyClass.__new__</code>返回的结果运行<code>MyClass.__init__</code></p>
</li>
<li><p>返回的对象已经准备完毕</p>
</li>
</ol>
</blockquote>
<p>以上就是Python创建一个实例对象的过程。</p>
<p>参考: <a href="http://www.cafepy.com/article/python_types_and_objects" target="_blank" rel="external">http://www.cafepy.com/article/python_types_and_objects</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Python的”类型-对象”体系实现的简洁而优雅,”元类”-“类”-“实例对象”自上而下,层次分明。&lt;br&gt;以下是关键的几个点:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1.一切(函数,类等)皆对象(&lt;code&gt;object&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;2.变量名只是对象
    
    </summary>
    
      <category term="Python" scheme="https://cyrusin.github.io/categories/Python/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>在Python中实现单例模式</title>
    <link href="https://cyrusin.github.io/2015/09/15/python-singleton-20150915/"/>
    <id>https://cyrusin.github.io/2015/09/15/python-singleton-20150915/</id>
    <published>2015-09-15T07:17:58.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>有些时候你的项目中难免需要一些全局唯一的对象,这些对象大多是一些工具性的东西,在Python中实现单例模式并不是什么难事。以下总结几种方法:</p>
<p>##使用类装饰器<br>使用装饰器实现单例类的时候,类本身并不知道自己是单例的,所以写代码的人可以不care这个,只要正常写自己的类的实现就可以,类的单例有装饰器保证。</p>
<pre><code>def singleton(cls):
    instances = {}
    def _wrapper(*args, **kwargs):
        if cls not in instances:
            instances[cls] = cls(*args, **kwargs)
        return instances[cls]
    return _wrapper
</code></pre><p>你会发现<code>singleton</code>装饰器内部使用了一个<code>dict</code>。当然你也可以用其他的方式,不过以下的实现是<strong>错误</strong>的:</p>
<pre><code>def singleton(cls):
    _instance = None #外部作用域的引用对于嵌套的内部作用域是只读的
    def _wrapper(*args, **kwargs):
        if _instance is None: #解释器会抛出&quot;UnboundLocalError: ...referenced before assignment&quot;
            _instance = cls(*args, **kwargs) #赋值行为使解释器将&quot;_instance&quot;看作局部变量
        return _instance
    return _wrapper 
</code></pre><p>##使用元类(<code>__metaclass__</code>)和可调用对象(<code>__call__</code>)<br>Python的对象系统中一些皆对象,类也不例外,可以称之为”类型对象”,比较绕,但仔细思考也不难:类本身也是一种对象,只不过这种对象很特殊,它表示某一种类型。是对象,那必然是实例化来的,那么谁实例化后是这种类型对象呢?也就是元类。</p>
<p>Python中,<code>class</code>关键字表示定义一个类对象,此时解释器会按一定规则寻找<code>__metaclass__</code>,如果找到了,就调用对应的元类实现来实例化该类对象;没找到,就会调用<code>type</code>元类来实例化该类对象。</p>
<p><code>__call__</code>是Python的魔术方法,Python的面向对象是”Duck type”的,意味着对象的行为可以通过实现协议来实现,可以看作是一种特殊的接口形式。某个类实现了<code>__call__</code>方法意味着该类的对象是可调用的,可以想像函数调用的样子。再考虑一下<code>foo=Foo()</code>这种实例化的形式,是不是很像啊。结合元类的概念,可以看出,<code>Foo</code>类是单例的,则在调用<code>Foo()</code>的时候每次都返回了同样的对象。而<code>Foo</code>作为一个类对象是单例的,意味着它的类(即生成它的元类)是实现了<code>__call__</code>方法的。所以可以如下实现:</p>
<pre><code>class Singleton(type):
    def __init__(cls, name, bases, attrs):
        super(Singleton, cls).__init__(name, bases, attrs)
        cls._instance = None
    def __call__(cls, *args, **kwargs):
        if cls._instance is None
            # 以下不要使用&apos;cls._instance = cls(*args, **kwargs)&apos;, 防止死循环,
            # cls的调用行为已经被当前&apos;__call__&apos;协议拦截了
            # 使用super(Singleton, cls).__call__来生成cls的实例
            cls._instance = super(Singleton, cls).__call__(*args, **kwargs)
        return cls._instance

class Foo(object): #单例类
    __metaclass__ = Singleton

&gt;&gt;&gt;a = Foo()
&gt;&gt;&gt;b = Foo()
&gt;&gt;&gt;a is b
&gt;&gt;&gt;True
&gt;&gt;&gt;a.x = 1
&gt;&gt;&gt;b.x
&gt;&gt;&gt;1
</code></pre><p>##使用<code>__new__</code><br><code>__init__</code>不是Python对象的构造方法,<code>__init__</code>只负责初始化实例对象,在调用<code>__init__</code>方法之前,会首先调用<code>__new__</code>方法生成对象,可以认为<code>__new__</code>方法充当了构造方法的角色。所以可以在<code>__new__</code>中加以控制,使得某个类只生成唯一对象。具体实现时可以实现一个父类,重载<code>__new__</code>方法,单例类只需要继承这个父类就好。</p>
<pre><code>class Singleton(object):
    def __new__(cls, *args, **kwargs):
        if not hasattr(cls, &apos;_instance&apos;):
            cls._instance = super(Singleton, cls).__new__(cls, *args, **kwargs)
        return cls._instance

class Foo(Singleton): #单例类
    a = 1
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有些时候你的项目中难免需要一些全局唯一的对象,这些对象大多是一些工具性的东西,在Python中实现单例模式并不是什么难事。以下总结几种方法:&lt;/p&gt;
&lt;p&gt;##使用类装饰器&lt;br&gt;使用装饰器实现单例类的时候,类本身并不知道自己是单例的,所以写代码的人可以不care这个,只要
    
    </summary>
    
      <category term="Python" scheme="https://cyrusin.github.io/categories/Python/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://cyrusin.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>开着跑车换轮子：基于版本号与互斥锁,在Web服务端安全高效的定向强制更新Memcached缓存</title>
    <link href="https://cyrusin.github.io/2015/09/08/cache20150908/"/>
    <id>https://cyrusin.github.io/2015/09/08/cache20150908/</id>
    <published>2015-09-08T07:21:39.000Z</published>
    <updated>2016-12-16T08:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>Memcached作为高性能的分布式内存对象缓存系统,在web服务里应用较广,和高性能的异步非阻塞服务器Tornado搭配起来可以大幅提高服务端的性能。</p>
<p>##应用Memcached缓存热点请求结果<br>我们给客户端提供api,通过json来返回http请求的结果,一般Web服务都是如此。由于用的是Tornado,所以逻辑上大概长这样:</p>
<blockquote>
<p>一个请求对应一个RequestHandler对象,RequestHandler类通过定义get/post方法来处理http请求,结果会通过write方法写到一个write_buffer里,最终有flush方法负责将write_buffer里的结果返回给客户端。</p>
</blockquote>
<p>缓存的话,我们一般是这样:</p>
<blockquote>
<p>从Tornado的RequestHandler继承一个类作为需要缓存数据的handler的基类,比如取名RequestHandlerCached,该基类里会优先走缓存去结果(write_buffer)，取不到结果时才走实际的get/post方法,然后把实际请求的结果(write_buffer)重新set回Memcached里,这样只要继承这个类,你的请求就被缓存了,当请求过来的时候可以先从缓存里取结果。</p>
</blockquote>
<p>##关键问题: 如何手动批量更新指定的Memcached缓存(强制更新指定的缓存,不能强制flush_all())<br>我们服务里有很多请求的处理大概是从CMS的接口里拿一些数据,处理后返回。CMS是用来控制内容的一些展示、推荐策略的,所以会有一定程度的周期性更新,CMS主要提供给编辑操作,编辑可以在事先约定好的框架下填入一些内容,比如某个主题、某些刚上线的节目等。当给这些含有CMS的东西的handler加上缓存之后,现在有一个问题:</p>
<blockquote>
<p>当编辑更新了上游CMS系统api的内容之后,所有调用了该CMS服务的下游服务又该怎么立即更新缓存,并且，这种立即更新绝对不能给当前服务带来太大的性能波动？<br>本质上就是个<strong>上、下游服务</strong>协同的问题,或者说是调用者和被调用者之间的协同问题。<br>因为<strong>Memcached并不像Redis那样支持键的模糊匹配</strong>,怎样才能针对一批key做到手动、批量、安全、及时的数据更新呢？</p>
</blockquote>
<p>之前的代码是直接提供一个接口,编辑需要更新缓存的时候点个按钮就好了, 清缓存方法是直接flush_all,比较粗暴,好几次编辑在高峰期清缓存的时候,直接把服务搞挂了。</p>
<p>后来我决定研究一下这个缓存策略,一开始想作服务隔离,凡是这些会手动清的缓存一律用单独的一个Memcached服务来缓存,但这样还是会出现更新某个服务的缓存的时候把其他的也一块儿干掉了！因为Memcached不支持键的模糊匹配。</p>
<p>对于Memcached不支持键的模糊匹配这个问题(准确说是键的命名空间),网上倒是有一个解决方案:</p>
<blockquote>
<p>给你的键加前缀,比如某个handler的键都有前缀A(类似一个版本号),这样缓存的数据的键可以附在A之后,形成了一个命名空间,当你需要清理这个handler的缓存的时候,直接A=A+1,然后在把数据的键放在后面,这样你拿这个键去Memcached里取数据的时候肯定取不到数据,就会走实际请求,这样相当于更新了缓存,而之前的旧数据还在Memcached里,只是你取不到而已,由于这些数据都有过期时间,让他们到时候过期就行了。</p>
</blockquote>
<p>这倒是看起来挺美的一个方案,但是细细想一下,这种方案对于那种偶尔来那么一两下的清缓存需求倒是可以,但是对于这种清缓存的操作都是有编辑控制的情况,有以下问题:</p>
<blockquote>
<p>1.一旦有编辑在一个时间段内执行多次更新缓存的操作的话,Memcached内部会有大量我们认为过期但是Memcached不认为过期的数据。内存使用有一定风险。</p>
<p>2.版本号的过期时间怎么控制,因为你的数据完全依赖于这个版本号,一旦这个版本号取不到,你的数据也就拿不到了,那这个时候你走正常请求之后更不更新缓存,可是更新缓存的时候你怎么知道新的版本号是谁？初始化一个,可是假如你这次没有取到版本号是由于Memcached没命中,并不是因为没有版本号怎么办？</p>
<p>3.存你版本号的缓存重启的时候怎么办？</p>
<p>4.第一次上线的时候,所有采取这种方案的线上缓存可就全都废了。这代价可太大了点儿。</p>
</blockquote>
<p>这种完全用键来控制数据更新的方案让你的数据对于这个版本号过于敏感,对于频繁操作的手动更新缓存的需求来讲,风险较大。那么我想:既然用键不行,是否可以用缓存的value做点改进？</p>
<p>Memcached是一个key-value数据库,但这不意味着我们只能有这样的<strong>思维定势</strong>：</p>
<blockquote>
<p>缓存数据data,序列化成一个字符串,搞个key,set到Memcached里,取的时候,拿key去Memcached里get出来,然后反序列化。</p>
</blockquote>
<p>在set数据的时候,完全可以赋予数据一个简单的结构,构造出一个稍微结构化点儿的数据类型,类似一个dict:</p>
<pre><code>{
    data: ..., #待缓存的数据
    some_key: ..., #用于数据做控制的一个键值对
}
</code></pre><p>对于以上的结构,我们在set数据的时候,将以上的数据序列化;取数据的时候,get出来,然后反序列化成一个dict,则<code>dict.get(&#39;data&#39;)</code>就是我们的原始数据。这样的结构有什么用呢？</p>
<p>回到我们的问题:</p>
<blockquote>
<p>如何手动<strong>定向</strong>更新一批缓存数据而不影响其他的缓存数据?</p>
</blockquote>
<p>最好是更新缓存的时候服务的整体性能不要受到太大影响,避免出现短时间内大量数据失效导致请求全部走实际get/post方法而打爆CMS的服务器。我提出了以下的解决方案,首先,缓存数据时构造简单的结构,就像上文给出的一样,我构造如下的结构:</p>
<pre><code>{
    api.raw_data: ..., #原始数据
    #以下是一个UNIX时间戳, 作为数据的版本号
    api.update_token: 14398700873,
}
</code></pre><p>具体策略如下:</p>
<blockquote>
<p>1.从RequestHandler继承一个新的handler基类:CachedRequestHandler,这个基类会为继承自他的子类生成各自唯一的update_token,使用key(<module名|handler子类名>)在手动更新缓存时候set到Memcached里;</module名|handler子类名></p>
<p>2.每一个继承自CachedRequestHandler基类的子类都有自己的专属update_token,对于缓存的数据,也有一个update_token,和原始数据一起,构造简单的缓存数据结构dict,序列化后set到Memcached里;</p>
<p>3.手动更新某个handler的缓存时,操作很简单:实际是调接口往Memcached里set一个这个handler的专属update_token,key就是上文提到的key,值就用当前时间戳。</p>
<p>4.对于某一个具体的请求,取缓存时,先取对应handler的专属update_token(同一个handler的所有请求共享一个),再取该请求的缓存数据(序列化的dict),把取出的数据反序列化,拿出update_token,和专属的update_token对比,若不一致则强制更新缓存,并把新的缓存数据的update_token设为专属update_token;</p>
<p>5.问题:绑定到handler的专属update_token的过期时间怎么设?答案是和数据的生存期一致就可以了,因为Memcached里的缓存活得最长也就自己生存期的时间。到时候,你不清他,他也过期了;</p>
<p>6.问题:既然只有手动更新缓存的时候会设update_token,那么平时怎么办,平时数据取不到handler的专属update_token?答案是取不到handler专属update_token就不用care,让数据通过过期时间自己刷新。可是缓存数据set到Memcached里的时候也需要自己的update_token啊,很简单,随便一个默认小数字,因为我们平时并不care这个update_token,只有在手动更新缓存时,才会比较handler的update_token和缓存数据的update_token,所以平时他是什么并不重要,平时数据只要care过期时间就可以了;</p>
<p>7.为什么update_token用unix时间戳？时间总是向未来演进的,某一个时刻的时间戳总是比之前的说有时间戳都大,时刻具有先天的唯一性,保证我们在手动更新缓存的时候,可以通过和数据的update_token比较来通知数据:不一致,你赶紧更新自己吧;即使我们在项目中对这个时间戳做了取整处理,也无妨,除非你两次操作的间隙小于1秒,这个误差完全可以接受;</p>
</blockquote>
<p>##互斥锁:不要让所有请求瞬间失效<br>根据之前的描述,当你调用清缓存的接口时,Memcached里会出现一个绑定到某个handler的update_token(相当于数据的最新版本号),此时,请求过来后发现取到了handler的专属update_token,知道该check一下自己的update_token看是否是最新的update_token了,现在的问题是:高并发的情况下,此handler的所有的请求都会失效,都会去请求后端,非常粗暴,系统的性能会有颠簸。</p>
<p>我们希望的是:平滑的更新我们的缓存,我们不需要大量数据失效,全都过来重新走get/post,重新set到Memcached,根本没必要,因为更新一次缓存的时间很短,我们可以牺牲一点实时性,仅仅是很小的牺牲,因为缓存更新是很短暂的时间,一旦更新成功,后续的请求又可以都走缓存了。所以我有做了一下优化:</p>
<blockquote>
<p>利用Memcached的add命令模拟一下互斥锁。Memcached的单个命令是原子的,即不存在任何中间状态,很符合获取锁/释放锁这样的严格的原子性操作。add命令会在Memcached里不存在某个key时,以指定的key存储一个值到Memcached里,返回True;而当存在key时,直接返回False。</p>
</blockquote>
<p>使用方法是:</p>
<blockquote>
<p>当发现数据的当前版本号(api.update_token)和handler的专属版本号(update_token)不一致的时候,开始试图更新Memcached。当多个有共同缓存key的数据想更新缓存时,通过add命令抢一把互斥锁,只允许抢到锁的请求更新缓存,其余未抢到锁的请求直接返回刚才取到的老数据。</p>
</blockquote>
<p>由于更新缓存是很快就可以完成的事情,所以仅仅损失一点点实时性。但却带来了很平滑的更新:不用担心编辑手抖了,不停的刷新缓存,我们每次都只有一个请求会实际走get/post,然后负责更新缓存,其他的都直接返回老数据,服务器的压力很小很小。</p>
<p>##总结<br>我们的确牺牲了一点点空间和时间,但却换来了更好的可控性,代码里也仅仅增加一点点比较大小之类的逻辑,对于我们这种web服务(或者说绝大多数web服务类型):</p>
<blockquote>
<p>后端性能瓶颈基本是I/O(数据库,文件,第三方api)</p>
</blockquote>
<p>增加的一点响应时间跟IO瓶颈来讲基本不是一个量级的,实践证明我的方案还是能解决问题的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Memcached作为高性能的分布式内存对象缓存系统,在web服务里应用较广,和高性能的异步非阻塞服务器Tornado搭配起来可以大幅提高服务端的性能。&lt;/p&gt;
&lt;p&gt;##应用Memcached缓存热点请求结果&lt;br&gt;我们给客户端提供api,通过json来返回http请求的
    
    </summary>
    
      <category term="Memcached" scheme="https://cyrusin.github.io/categories/Memcached/"/>
    
    
      <category term="技术" scheme="https://cyrusin.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Tornado" scheme="https://cyrusin.github.io/tags/Tornado/"/>
    
      <category term="Memcached" scheme="https://cyrusin.github.io/tags/Memcached/"/>
    
  </entry>
  
</feed>
